{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some experiments with different version of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From annotated_diffusion \n",
    "#\n",
    "class Attention_1(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        print('hidden_dim:', hidden_dim)\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        print('q shape:', q.shape, ', k shape:', k.shape, ', v shape:', v.shape)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        print('torch.max(sim):', torch.max(sim), ', torch.min(sim):', torch.min(sim))\n",
    "        print('q*vt:  sim shape:', sim.shape)\n",
    "        print('sim.amax:', sim.amax(dim=-1, keepdim=True).shape)\n",
    "\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        print('torch.max(sim):', torch.max(sim), ', torch.min(sim):', torch.min(sim))\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        print('attn shape:', attn.shape)\n",
    "        print('torch.min(attn):', torch.min(attn), ', torch.max(attn):', torch.max(attn))\n",
    "    \n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        print('out shape:', out.shape)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        print('out shape:', out.shape)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From my code\n",
    "class Attention_2(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dim_head=32, numgroups=8, dropout=0.):  \n",
    "        super().__init__()        \n",
    "        inner_dim = dim_head * num_heads\n",
    "        print('inner_dim:', inner_dim)\n",
    "        project_out = not (num_heads == 1 and dim_head == dim)\n",
    "        self.heads = num_heads\n",
    "        self.attention_norm = nn.GroupNorm(numgroups, dim)\n",
    "        self.scale = float(dim_head) ** -0.5\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)  #use Conv2d instead of Linear????\n",
    "        # self.attn_dropout = nn.Dropout(dropout)  # Don't do dropout\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),  #can use conv2d instead of Linear\n",
    "            # nn.Dropout(dropout)   # Don't do dropout\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        in_attn = x.reshape(b, c, h * w)\n",
    "        # GroupNorm applies only to the c channels, so the dimensions of the tensor \n",
    "        # after that is probably not important either way\n",
    "        in_attn = self.attention_norm(in_attn) \n",
    "        print('in_attn shape:', in_attn.shape)\n",
    "        in_attn = in_attn.transpose(1,2)\n",
    "        print('in_attn shape:', in_attn.shape)\n",
    "        qkv = self.to_qkv(in_attn).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        print('q shape:', q.shape, ', k shape:', k.shape, ', v shape:', v.shape)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        print('q*vt shape:', dots.shape)\n",
    "        attn = self.attend(dots)\n",
    "        # attn = self.attn_dropout(attn)  Don't do dropout\n",
    "        out = torch.matmul(attn, v)\n",
    "        print('out shape:', out.shape)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        print('out shape:', out.shape)\n",
    "        out = self.to_out(out)\n",
    "        print('out shape:', out.shape)\n",
    "        out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "        print('out shape:', out.shape)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses pytorch's MultiheadAttention\n",
    "\n",
    "class Attention_3(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, numgroups=8, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.attention_norms = nn.GroupNorm(numgroups, dim)\n",
    "        self.attentions = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        # Attention block of Unet\n",
    "        batch_size, channels, h, w = out.shape\n",
    "        in_attn = out.reshape(batch_size, channels, h * w)\n",
    "        in_attn = self.attention_norms(in_attn)\n",
    "        in_attn = in_attn.transpose(1, 2)    #So, I guess: [N, (h*w), C] where (h*w) is the target \"sequence length\", and C is the embedding dimension\n",
    "        out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n",
    "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "        return out_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From annotated_diffusion\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        print('hidden_dim:', hidden_dim)\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        print('q shape:', q.shape, ', k shape:', k.shape, ', v shape:', v.shape)\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "        print('context shape:', context.shape)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        print('out shape:', out.shape)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        print('out shape:', out.shape) \n",
    "        out = self.to_out(out)\n",
    "        print('out shape:', out.shape) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in x shape: torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([2, 64, 32, 32])\n",
    "print('in x shape:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dim: 128\n",
      "q shape: torch.Size([2, 4, 32, 1024]) , k shape: torch.Size([2, 4, 32, 1024]) , v shape: torch.Size([2, 4, 32, 1024])\n",
      "torch.max(sim): tensor(2.0663, grad_fn=<MaxBackward1>) , torch.min(sim): tensor(-2.1919, grad_fn=<MinBackward1>)\n",
      "q*vt:  sim shape: torch.Size([2, 4, 1024, 1024])\n",
      "sim.amax: torch.Size([2, 4, 1024, 1])\n",
      "torch.max(sim): tensor(0., grad_fn=<MaxBackward1>) , torch.min(sim): tensor(-4.1100, grad_fn=<MinBackward1>)\n",
      "attn shape: torch.Size([2, 4, 1024, 1024])\n",
      "torch.min(attn): tensor(9.2696e-05, grad_fn=<MinBackward1>) , torch.max(attn): tensor(0.0070, grad_fn=<MaxBackward1>)\n",
      "out shape: torch.Size([2, 4, 1024, 32])\n",
      "out shape: torch.Size([2, 128, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "attn1 = Attention_1(64, 4, 32)\n",
    "out = attn1(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_dim: 128\n",
      "in_attn shape: torch.Size([2, 64, 1024])\n",
      "in_attn shape: torch.Size([2, 1024, 64])\n",
      "q shape: torch.Size([2, 4, 1024, 32]) , k shape: torch.Size([2, 4, 1024, 32]) , v shape: torch.Size([2, 4, 1024, 32])\n",
      "q*vt shape: torch.Size([2, 4, 1024, 1024])\n",
      "out shape: torch.Size([2, 4, 1024, 32])\n",
      "out shape: torch.Size([2, 1024, 128])\n",
      "out shape: torch.Size([2, 1024, 64])\n",
      "out shape: torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "attn2 = Attention_2(64, 4, 32)\n",
    "out = attn2(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "attn3 = Attention_3(64, 4)\n",
    "out = attn3(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dim: 128\n",
      "q shape: torch.Size([2, 4, 32, 1024]) , k shape: torch.Size([2, 4, 32, 1024]) , v shape: torch.Size([2, 4, 32, 1024])\n",
      "context shape: torch.Size([2, 4, 32, 32])\n",
      "out shape: torch.Size([2, 4, 32, 1024])\n",
      "out shape: torch.Size([2, 128, 32, 32])\n",
      "out shape: torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "attn4 = LinearAttention(64, 4, 32)\n",
    "out = attn4(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "heads = 4\n",
    "dim_head =128\n",
    "inner_dim = dim_head *  heads\n",
    "numgroups = 8\n",
    "\n",
    "x = torch.randn([2, 256, 32, 32])\n",
    "print('in x shape:', x.shape)\n",
    "\n",
    "b, c, h, w = x.shape\n",
    "norm = nn.GroupNorm(numgroups, dim)\n",
    "in_attn = norm(x)\n",
    "in_attn = x.reshape(b, h * w, c)\n",
    "# in_attn = in_attn.transpose(1, 2)  # reshape to [b, (h*w), c] i.e. [b, seq, emb_dim]\n",
    "print('in_attn shape:', in_attn.shape)\n",
    "\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.normal_(to_qkv.weight.data, mean=0., std=np.sqrt(2 / (dim+inner_dim)))\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.xavier_normal_(to_qkv.weight.data)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "\n",
    "\n",
    "qkv = to_qkv(in_attn)\n",
    "print('out shape:', qkv.shape)\n",
    "\n",
    "qkv = qkv.chunk(3, dim = -1)\n",
    "print('q shape:', qkv[0].shape)\n",
    "\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print('q shape:', q.shape)\n",
    "\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) \n",
    "print('dots shape:', dots.shape)\n",
    "\n",
    "out = torch.matmul(dots, v)\n",
    "print('1 out shape:', out.shape)\n",
    "\n",
    "out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "print('2 out shape:', out.shape)\n",
    "\n",
    "to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "out = to_out(out)\n",
    "print('3 out shape:', out.shape)\n",
    "\n",
    "out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "print('4 out shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
