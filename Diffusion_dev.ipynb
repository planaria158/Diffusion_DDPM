{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some misc. code snippets while learning diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from celeba_dataset import CelebA\n",
    "from unet_diffusion import UNet_Diffusion, get_time_embedding\n",
    "from noise_scheduler import LinearNoiseScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (128,128) \n",
    "batch_size = 8 \n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "lns = LinearNoiseScheduler(num_timesteps, beta_start, beta_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create two random vectors and interpolate between them.\n",
    "# rand_a = torch.randn(3, 64, 64)\n",
    "# rand_b = torch.randn(3, 64, 64)\n",
    "# delta_ab = rand_a - rand_b\n",
    "# print(delta_ab.shape)\n",
    "# num_samples = 10\n",
    "\n",
    "# samples = []\n",
    "# samples.append(rand_a)\n",
    "# delt = 1.0/num_samples\n",
    "# for i in range(1, 9, 1):\n",
    "#     s = rand_a + (i * delt) * delta_ab\n",
    "#     samples.append(s)\n",
    "\n",
    "# samples.append(rand_b)\n",
    "# print('len(samples):', len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restarting from checkpoint\n",
      "on_load_checkpoint: calling self.ema.step: 330513\n",
      "Model has: 158 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 1 has a total capacty of 10.75 GiB of which 9.78 GiB is free. Including non-PyTorch memory, this process has 988.00 MiB memory in use. Of the allocated memory 712.73 MiB is allocated by PyTorch, and 77.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 99\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#----------------------------------------------------\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Run the inference\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#----------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36minfer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m LinearNoiseScheduler(num_timesteps\u001b[38;5;241m=\u001b[39mnum_timesteps,\n\u001b[1;32m     87\u001b[0m                                  beta_start\u001b[38;5;241m=\u001b[39mbeta_start,\n\u001b[1;32m     88\u001b[0m                                  beta_end\u001b[38;5;241m=\u001b[39mbeta_end)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 90\u001b[0m     \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m, in \u001b[0;36msample\u001b[0;34m(model, scheduler)\u001b[0m\n\u001b[1;32m     48\u001b[0m xt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((num_samples, im_channels, im_size, im_size))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(num_timesteps))):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Get prediction of noise\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Use scheduler to get x0 and xt-1\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     xt, x0_pred \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39msample_prev_timestep(xt, noise_pred, torch\u001b[38;5;241m.\u001b[39mas_tensor(i)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/diffusion/unet_diffusion.py:268\u001b[0m, in \u001b[0;36mUNet_Diffusion.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_in(x)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m#------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m#------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m enc_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m enc_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_1(enc_0, t_emb)\n\u001b[1;32m    270\u001b[0m enc_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_2(enc_1, t_emb)\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/diffusion/unet_diffusion.py:139\u001b[0m, in \u001b[0;36mDownBlock.forward\u001b[0;34m(self, x, t_emb)\u001b[0m\n\u001b[1;32m    136\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_block_1(x, t_emb)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention:\n\u001b[0;32m--> 139\u001b[0m     out_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m out_attn  \n\u001b[1;32m    142\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_block_2(x, t_emb)\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/diffusion/unet_diffusion.py:113\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m in_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_norms(in_attn)\n\u001b[1;32m    112\u001b[0m in_attn \u001b[38;5;241m=\u001b[39m in_attn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)    \u001b[38;5;66;03m#So, I guess: [N, (h*w), C] where (h*w) is the target \"sequence length\", and C is the embedding dimension\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m out_attn, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattentions\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_attn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m out_attn \u001b[38;5;241m=\u001b[39m out_attn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, channels, h, w)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_attn\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/avm-dvm/lib/python3.9/site-packages/torch/nn/modules/activation.py:1196\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_masks(attn_mask, key_padding_mask, query)\n\u001b[1;32m   1195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_native_multi_head_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m                \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m                \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m                \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1211\u001b[0m any_nested \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m key\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_nested\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_nested, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1213\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe fast path was not hit because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhy_not_fast_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 1 has a total capacty of 10.75 GiB of which 9.78 GiB is free. Including non-PyTorch memory, this process has 988.00 MiB memory in use. Of the allocated memory 712.73 MiB is allocated by PyTorch, and 77.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "time_emb_dim = 256 #128\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "from torchvision.utils import make_grid\n",
    "from unet_diffusion import UNet_Diffusion\n",
    "from diffusion_lightning import DDPM\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_grid_rows = 1\n",
    "num_samples = num_grid_rows * num_grid_rows\n",
    "im_channels = 3\n",
    "im_size = img_size[0]\n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "task_name = 'default'\n",
    "ckpt_name = 'model_ckpt.pth'\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def sample(model, scheduler):\n",
    "    \"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    We save the x0 predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # # Create two random vectors and interpolate between them.\n",
    "    # rand_a = torch.randn(im_channels, im_size, im_size)\n",
    "    # rand_b = torch.randn(im_channels, im_size, im_size)\n",
    "    # delta_ab = rand_a - rand_b\n",
    "    # samples = []\n",
    "    # samples.append(rand_a)\n",
    "    # delt = 1.0/num_samples\n",
    "    # for i in range(1, (num_samples-1), 1):\n",
    "    #     s = rand_a + (i * delt) * delta_ab\n",
    "    #     samples.append(s)\n",
    "\n",
    "    # samples.append(rand_b)\n",
    "    # xt = torch.stack(samples).to(device)\n",
    "    # print('xt shape:', xt.shape)\n",
    "\n",
    "    xt = torch.randn((num_samples, im_channels, im_size, im_size)).to(device)\n",
    "\n",
    "    for i in tqdm(reversed(range(num_timesteps))):\n",
    "        # Get prediction of noise\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "        \n",
    "        # Save x0 every 200th time.\n",
    "        if i % 200 == 0 or (i == num_timesteps-1):\n",
    "            ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "            ims = (ims + 1) / 2\n",
    "            grid = make_grid(ims, nrow=num_grid_rows)\n",
    "            img = torchvision.transforms.ToPILImage()(grid)\n",
    "            if not os.path.exists(os.path.join(task_name, 'samples')):\n",
    "                os.mkdir(os.path.join(task_name, 'samples'))\n",
    "            img.save(os.path.join(task_name, 'samples', 'x0_{}.png'.format(i)))\n",
    "            img.close()\n",
    "\n",
    "\n",
    "def infer():\n",
    "    map_location = {'cuda:0':'cuda:1'}\n",
    "    model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_0/checkpoints/epoch=28-step=330513.ckpt',\n",
    "                                      map_location=map_location)\n",
    "    \n",
    "    model.ema_model = None # dump the extra EMA model (to reduce memory footprint)\n",
    "\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    print('Model has:', int(total_params//1e6), 'M parameters')\n",
    "\n",
    "    \n",
    "    # model = UNet_Diffusion(time_emb_dim).to(device)\n",
    "    # model.load_state_dict(torch.load(os.path.join(task_name, ckpt_name), map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=num_timesteps,\n",
    "                                     beta_start=beta_start,\n",
    "                                     beta_end=beta_end)\n",
    "    with torch.no_grad():\n",
    "        sample(model.model, scheduler)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Run the inference\n",
    "#----------------------------------------------------\n",
    "infer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "\n",
    "# Verify the mean and the variance: \n",
    "abs(mu - np.mean(s))\n",
    "0.0  # may vary\n",
    "\n",
    "abs(sigma - np.std(s, ddof=1))\n",
    "0.1  # may vary\n",
    "\n",
    "\n",
    "# Display the histogram of the samples, along with the probability density function:\n",
    "count, bins, ignored = plt.hist(s, 30, density=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "from torchvision.transforms.v2 import RandomResizedCrop, RandomRotation, GaussianBlur, RandomErasing\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Dataset, Dataloader\n",
    "#--------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "image_dir_train = Path('../data/img_align_celeba/img_align_celeba/')\n",
    "\n",
    "img_size = (64,64) \n",
    "batch_size = 8 \n",
    "\n",
    "\n",
    "train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "                            RandomHorizontalFlip(p=0.50),\n",
    "                            # RandomVerticalFlip(p=0.25),\n",
    "                            # transforms.RandomApply(nn.ModuleList([GaussianBlur(kernel_size=7)]), p=0.5),\n",
    "                            # transforms.RandomApply(nn.ModuleList([RandomRotation(10.0)]), p=0.5),\n",
    "                            # RandomResizedCrop(size=img_size, scale=(0.3, 1.0), antialias=True),\n",
    "                            # RandomErasing(p=0.5, scale=(0.02, 0.20)),\n",
    "                            Resize(img_size, antialias=True)\n",
    "                            ])\n",
    "\n",
    "train_dataset = CelebA(image_dir_train, transform=train_transforms)\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=5, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self) : #, mean, std):\n",
    "        pass\n",
    "    def __call__(self, img):\n",
    "        img = (img*127.5) + 127.5\n",
    "        return img\n",
    "    \n",
    "unorm  = UnNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, _  = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(torch.min(images[0]), ', ', torch.max(images[0]))\n",
    "\n",
    "\n",
    "cols = 4\n",
    "rows = 4\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(10, 10))\n",
    "idx = 0\n",
    "for img in (images):  \n",
    "    img = unorm(img).to(torch.uint8).permute(1, 2, 0)\n",
    "    # target = unorm(target).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "    idx += 1\n",
    "    ax = plt.subplot(rows, cols, idx)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    if idx == (cols*rows):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_0, _  = next(iter(train_loader))\n",
    "shape = images_0.shape\n",
    "print(shape)\n",
    "noise = torch.randn(shape[2], shape[3])\n",
    "print(noise.shape)\n",
    "print(images[0:5].shape)\n",
    "\n",
    "imgs_n = lns.add_noise(images[0:1], noise, 50)\n",
    "print(imgs_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 2\n",
    "rows = 1\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(5, 5))\n",
    "idx = 0\n",
    "\n",
    "img   = unorm(images[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "img_n = unorm(imgs_n[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb_dim = 128\n",
    "time_steps = torch.ones((512)) * 999\n",
    "print(time_steps.shape)\n",
    "\n",
    "blah = time_steps[:, None]\n",
    "print(blah.shape)\n",
    "\n",
    "poo = blah.repeat(1, 128//2)\n",
    "print(poo.shape)\n",
    "\n",
    "\n",
    "t_emb = get_time_embedding(time_steps, time_emb_dim)\n",
    "print(t_emb.shape)\n",
    "print(t_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------\n",
    "#\n",
    "# DDPM Diffusion Model\n",
    "# as a pytorch lightning module.\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import copy\n",
    "\n",
    "from unet_diffusion import UNet_Diffusion\n",
    "from noise_scheduler import LinearNoiseScheduler\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Exponential moving average for more stable training\n",
    "# copied from https://github.com/dome272/Diffusion-Models-pytorch\n",
    "# -------------------------------------------------------------------\n",
    "class EMA:\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
    "        if self.step < step_start_ema:\n",
    "            self.reset_parameters(ema_model, model)\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average(ema_model, model)\n",
    "        self.step += 1\n",
    "\n",
    "    def reset_parameters(self, ema_model, model):\n",
    "        ema_model.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "class DDPM(LightningModule):\n",
    "    def __init__(self,\n",
    "                **kwargs):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.num_timesteps = 1000\n",
    "        self.beta_start = 0.0001\n",
    "        self.beta_end = 0.02\n",
    "        self.time_emb_dim = 256\n",
    "        self.num_epochs = 500\n",
    "        self.model = UNet_Diffusion(self.time_emb_dim)\n",
    "        self.scheduler = LinearNoiseScheduler(self.num_timesteps, self.beta_start, self.beta_end)\n",
    "        self.ema = EMA(0.995)\n",
    "        self.ema_model = copy.deepcopy(self.model).eval().requires_grad_(False)\n",
    "\n",
    "        # print('self.optimizers:', self.optimizers)\n",
    "        # print('self.lr_schedulers:', self.lr_schedulers)\n",
    "        print('self.current_epoch:', self.current_epoch)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, noisy_im, t):\n",
    "        return self.model(noisy_im, t)\n",
    "    \n",
    "    def common_forward(self, batch):\n",
    "        imgs = batch[0]\n",
    "        # Random noise\n",
    "        noise = torch.randn_like(imgs) \n",
    "        # Timestep\n",
    "        tstep = torch.randint(0, self.num_timesteps, (imgs.shape[0],)) \n",
    "        # Add noise to images according to timestep\n",
    "        noisy_imgs = self.scheduler.add_noise(imgs, noise, tstep).to(imgs)\n",
    "        # Model tries to learn the noise that was added to im to make noise_im\n",
    "        noise_pred = self.forward(noisy_imgs, tstep.to(imgs))\n",
    "        # Loss is our predicted noise relative to actual noise\n",
    "        loss = self.criterion(noise_pred, noise)\n",
    "        return loss\n",
    "    \n",
    "    # ---------------------------------------------------------------\n",
    "    # Training step:\n",
    "    # ---------------------------------------------------------------\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.common_forward(batch)\n",
    "        self.log_dict({\"loss\": loss}, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        # After every batch, apply the EMA-based weights update\n",
    "        self.ema.step_ema(self.ema_model, self.model)\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Validation step:\n",
    "    # ---------------------------------------------------------------\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss = self.common_forward(batch)\n",
    "        self.log_dict({\"val_loss\": val_loss}, prog_bar=True, sync_dist=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        print(\"\\nRestarting from checkpoint\")\n",
    "        print(type(checkpoint))\n",
    "        print(checkpoint.keys())\n",
    "        print('epoch:', checkpoint['epoch'])\n",
    "        print('global_step:', checkpoint['global_step'])\n",
    "        print('lr_schedulers:', checkpoint['lr_schedulers'])\n",
    "        print('loops:', checkpoint['loops'])\n",
    "        print('hyper_parameters:', checkpoint['hyper_parameters'])\n",
    "        print('type(optimizer_states):', type(checkpoint['optimizer_states'][0]))\n",
    "        print('self.current_epoch;', self.current_epoch)\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "\n",
    "        self.ema_model = copy.deepcopy(self.model).eval().requires_grad_(False)\n",
    "        self.ema.step = checkpoint['global_step'] \n",
    "        print('on_load_checkpoint: calling self.ema.step:', self.ema.step)\n",
    "\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print('calling configure_optimizers')\n",
    "        lr = 0.0002  \n",
    "        b1 = 0.5\n",
    "        b2 = 0.999\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, betas=(b1, b2))\n",
    "        # I have no evidence to suggest scheduler is an improvement, but let's give it a whirl anyway :)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_location = {'cuda:0':'cuda:1'}\n",
    "model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_10/checkpoints/epoch=3-step=72936.ckpt',\n",
    "                                  map_location=map_location) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1) \n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=10,\n",
    "    every_n_epochs=1,\n",
    "    monitor = 'loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "map_location = {'cuda:0':'cuda:1'}\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=\"lightning_logs\", default_hp_metric=False)\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=500,\n",
    "                     logger=logger, log_every_n_steps=1000, callbacks=[checkpoint_callback],\n",
    "                     checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_10/checkpoints/epoch=3-step=72936.ckpt') \n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Misc debugging code for the AttentionBlock (pytorch's vs. the hand-crafted version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\ninput x shape:', x.shape)\n",
    "# print('in_attn shape:', in_attn.shape)\n",
    "# print('qkv, len:', len(qkv), ', qkv[0] shape:', qkv[0].shape)\n",
    "# print('q shape:', q.shape, ', k shape:', k.shape, ', v shape:', v.shape)\n",
    "# print('attn shape:', attn.shape, ', size MB:', ((attn.shape[-2] * attn.shape[-1]) * 8)/1e6 )\n",
    "# print('1. out_attn shape:', out_attn.shape)\n",
    "# print('2. out_attn shape:', out_attn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "class AttentionBlock_new(nn.Module):\n",
    "    # When used to process image batches [b, c, h, w]: \n",
    "    #     emb_dim == c, the number of channels\n",
    "    #     sequence == (h*w), the logical sequence\n",
    "    #\n",
    "    #  Images reshaped to [b, (h*w), c]\n",
    "    #  Wq, Wk, Wv weight matrices will each be: [emb_dim, 3*(heads * dim_head)]\n",
    "    #\n",
    "    def __init__(self, emb_dim, num_heads = 4, numgroups=8, dropout = 0, bias=False):  \n",
    "        super().__init__()\n",
    "        assert emb_dim % numgroups == 0  # must divide equally\n",
    "        assert emb_dim % num_heads == 0  # must divide equally\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_norm = nn.GroupNorm(numgroups, emb_dim)\n",
    "        inner_dim = emb_dim \n",
    "        project_out = not (num_heads == 1)\n",
    "        self.scale = emb_dim ** -0.5  #dim_head ** -0.5\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "\n",
    "        # This makes the Wq,Wk,Wv weight matrices\n",
    "        self.to_qkv = nn.Linear(emb_dim, inner_dim * 3, bias) #False) # ?? maybe should be True?\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "# ????\n",
    "#         self._reset_parameters()\n",
    "\n",
    "# ????\n",
    "#     def _reset_parameters(self):\n",
    "#         # Original Transformer initialization, see PyTorch documentation\n",
    "#         nn.init.xavier_uniform_(self.to_qkv.weight)\n",
    "#         self.to_qkv.bias.data.fill_(0)\n",
    "#         nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "#         self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('\\ninput x shape:', x.shape)\n",
    "        b, c, h, w = x.shape\n",
    "        # in_attn = x.reshape(b, c, h * w).transpose(1, 2) \n",
    "        in_attn = self.attention_norm(x)\n",
    "        in_attn = x.reshape(b, h * w, c)\n",
    "        # in_attn = in_attn.transpose(1, 2)  # reshape to [b, (h*w), c] i.e. [b, seq, emb_dim]\n",
    "        print('in_attn shape:', in_attn.shape)\n",
    "\n",
    "        qkv = self.to_qkv(in_attn).chunk(3, dim = -1)\n",
    "        print('qkv, len:', len(qkv), ', qkv[0] shape:', qkv[0].shape)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.num_heads), qkv) # [Batch, Head, SeqLen, Dims]\n",
    "        print('q shape:', q.shape, ', k shape:', k.shape, ', v shape:', v.shape)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        print('attn shape:', attn.shape, ', size MB:', ((attn.shape[-2] * attn.shape[-1]) * 8)/1e6 )\n",
    "        out_attn = torch.matmul(attn, v)\n",
    "        print('1. out_attn shape:', out_attn.shape)  # [2, 4, 1024, 64]\n",
    "        out_attn = rearrange(out_attn, 'b h n d -> b n (h d)')\n",
    "        print('2. out_attn shape:', out_attn.shape)  # [2, 1024, 256]\n",
    "        out_attn = out_attn.transpose(1, 2).reshape(b, c, h, w)\n",
    "        print('4. out_attn shape:', out_attn.shape)\n",
    "        return out_attn  \n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, out_channels, num_heads=4, numgroups=8):\n",
    "        super().__init__()\n",
    "        self.attention_norms = nn.GroupNorm(numgroups, out_channels)\n",
    "        self.attentions = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        # Attention block of Unet\n",
    "        batch_size, channels, h, w = out.shape\n",
    "        in_attn = out.reshape(batch_size, channels, h * w)\n",
    "        in_attn = self.attention_norms(in_attn)\n",
    "        in_attn = in_attn.transpose(1, 2)    #So, I guess: [N, (h*w), C] where (h*w) is the target \"sequence length\", and C is the embedding dimension\n",
    "        out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n",
    "        print('\\nout_attn shape:', out_attn.shape)\n",
    "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "        return out_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "emb_dim = c = 256\n",
    "h = 32\n",
    "w = 32\n",
    "groups = 8\n",
    "heads = 4\n",
    "dim_head = emb_dim//heads\n",
    "dropout = 0\n",
    "x = torch.randn((b, c, h, w))\n",
    "print('input shape:', x.shape)\n",
    "\n",
    "attn1 = AttentionBlock_new(emb_dim, heads)\n",
    "attn2 = AttentionBlock(c, heads, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, :, None, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = attn1.forward(x)\n",
    "print(out1.shape, ', mean:', torch.mean(out1), ', std:', torch.std(out1))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out2 = attn2.forward(x)\n",
    "print(out2.shape, ', mean:', torch.mean(out2), ', std:', torch.std(out2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "batch = torch.randn([32, 3, 128, 128])\n",
    "make_patches = nn.Conv2d(3, 3, 16, 16, padding=1)\n",
    "\n",
    "patches = make_patches(batch)\n",
    "print('patches shape:', patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
