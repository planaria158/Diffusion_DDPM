{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some misc. code snippets while learning diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from celeba_dataset import CelebA\n",
    "from unet_diffusion import UNet_Diffusion, get_time_embedding\n",
    "from noise_scheduler import LinearNoiseScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (64,64) \n",
    "batch_size = 8 \n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "lns = LinearNoiseScheduler(num_timesteps, beta_start, beta_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "### Experiments that give good results:\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Experiment 1\n",
    "\n",
    "-  No augmentations other than horizontal flips (do NOT use Gaussian blur from pytorch packages since this messes up the scheduled noising of the images)\n",
    "-  The AttentionBlock uses nn.MultiheadAttention with #heads = 4\n",
    "-  Attention=True only for self.down_1, self.down_2, self.up_2, self.up_1 (all others false)\n",
    "-  img_shape = (64,64), batch_size=10, two-gpu strategy='ddp_find_unused_parameters_true'\n",
    "-  time_emb dimension = 256\n",
    "-  Epochs = 26 (~250K batches of 10 images)\n",
    "-  num_timesteps = 1000, beta_start = 0.0001, beta_end = 0.02\n",
    "-  Exponential moving average with warmup of 2000 batches\n",
    "-  Adam optimizer, lr = 0.0002, b1 = 0.5, b2 = 0.999\n",
    "-  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "\n",
    "| <img src=\"images/x0_0_save11_works_again.png\" alt=\"\" width=\"300\"/> | \n",
    "|:--:| \n",
    "\n",
    "<hr>\n",
    "\n",
    "### Experiment 2\n",
    "\n",
    "-  The AttentionBlock uses my multiheaded self attention code instead of pytorch's code.\n",
    "-  Number of heads = 12\n",
    "-  No augmentations other than horizontal flips\n",
    "-  Attention=True only for self.down_1, self.down_2, self.up_2, self.up_1 (all others false)\n",
    "-  img_shape = (64,64), batch_size=80, two-gpu strategy='ddp_find_unused_parameters_true'\n",
    "-  time_emb dimension = 256\n",
    "-  Epochs ~40 \n",
    "-  num_timesteps = 1000, beta_start = 0.0001, beta_end = 0.02\n",
    "-  Exponential moving average with warmup of 2000 batches\n",
    "-  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "\n",
    "| <img src=\"images/x0_0_save13_newAttn_more_fully_conv_12heads.png\" alt=\"\" width=\"300\"/> | \n",
    "|:--:| \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restarting from checkpoint\n",
      "on_load_checkpoint: calling self.ema.step: 33060\n",
      "Model has: 150 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:50, 19.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "time_emb_dim = 256 #128\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "from torchvision.utils import make_grid\n",
    "from unet_diffusion import UNet_Diffusion\n",
    "from diffusion_lightning import DDPM\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_samples = 25\n",
    "num_grid_rows = 5\n",
    "im_channels = 3\n",
    "im_size = img_size[0]\n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "task_name = 'default'\n",
    "ckpt_name = 'model_ckpt.pth'\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def sample(model, scheduler):\n",
    "    \"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    We save the x0 predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # # Create two random vectors and interpolate between them.\n",
    "    # rand_a = torch.randn(im_channels, im_size, im_size)\n",
    "    # rand_b = torch.randn(im_channels, im_size, im_size)\n",
    "    # delta_ab = rand_a - rand_b\n",
    "    # samples = []\n",
    "    # samples.append(rand_a)\n",
    "    # delt = 1.0/num_samples\n",
    "    # for i in range(1, (num_samples-1), 1):\n",
    "    #     s = rand_a + (i * delt) * delta_ab\n",
    "    #     samples.append(s)\n",
    "\n",
    "    # samples.append(rand_b)\n",
    "    # xt = torch.stack(samples).to(device)\n",
    "    # print('xt shape:', xt.shape)\n",
    "\n",
    "    xt = torch.randn((num_samples, im_channels, im_size, im_size)).to(device)\n",
    "\n",
    "    for i in tqdm(reversed(range(num_timesteps))):\n",
    "        # Get prediction of noise\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "        \n",
    "        # Save x0 every 200th time.\n",
    "        if i % 200 == 0 or (i == num_timesteps-1):\n",
    "            ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "            ims = (ims + 1) / 2\n",
    "            grid = make_grid(ims, nrow=num_grid_rows)\n",
    "            img = torchvision.transforms.ToPILImage()(grid)\n",
    "            if not os.path.exists(os.path.join(task_name, 'samples')):\n",
    "                os.mkdir(os.path.join(task_name, 'samples'))\n",
    "            img.save(os.path.join(task_name, 'samples', 'x0_{}.png'.format(i)))\n",
    "            img.close()\n",
    "\n",
    "\n",
    "def infer():\n",
    "    # map_location = {'cuda:0':'cuda:1'}\n",
    "    model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_9/checkpoints/epoch=28-step=33060.ckpt') #,\n",
    "                                    #   map_location=map_location)\n",
    "    \n",
    "    model.ema_model = None # dump the extra EMA model (to reduce memory footprint)\n",
    "\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    print('Model has:', int(total_params//1e6), 'M parameters')\n",
    "\n",
    "    \n",
    "    # model = UNet_Diffusion(time_emb_dim).to(device)\n",
    "    # model.load_state_dict(torch.load(os.path.join(task_name, ckpt_name), map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=num_timesteps,\n",
    "                                     beta_start=beta_start,\n",
    "                                     beta_end=beta_end)\n",
    "    with torch.no_grad():\n",
    "        sample(model.model, scheduler)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Run the inference\n",
    "#----------------------------------------------------\n",
    "infer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Misc debugging code for the AttentionBlock (pytorch's vs. the hand-crafted version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     print('\\ninput x shape:', x.shape)\n",
    "    #     b, c, h, w = x.shape\n",
    "    #     in_attn = self.attention_norm(x)\n",
    "    #     in_attn = x.reshape(b, h * w, c)\n",
    "    #     # in_attn = in_attn.transpose(1, 2)  # reshape to [b, (h*w), c] i.e. [b, seq, emb_dim]\n",
    "    #     print('in_attn shape:', in_attn.shape)\n",
    "\n",
    "    #     qkv = self.to_qkv(in_attn).chunk(3, dim = -1)\n",
    "    #     print('qkv, len:', len(qkv), ', qkv[0] shape:', qkv[0].shape)\n",
    "    #     q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "    #     print('q shape:', q.shape, ', k shape:', k.shape, ', v shape:', v.shape)\n",
    "    #     dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "    #     print('q*k dot product shape:', dots.shape)\n",
    "    #     attn = self.attend(dots)\n",
    "    #     print('attn shape:', attn.shape)\n",
    "    #     out = torch.matmul(attn, v)\n",
    "    #     print('1. out shape:', out.shape)\n",
    "    #     out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "    #     out = self.to_out(out)\n",
    "    #     out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "    #     print('2. out shape:', out.shape)\n",
    "    #     return out \n",
    "\n",
    "    # def _reset_parameters(self):\n",
    "    #         if self._qkv_same_embed_dim:\n",
    "    #             xavier_uniform_(self.in_proj_weight)\n",
    "    #         else:\n",
    "    #             xavier_uniform_(self.q_proj_weight)\n",
    "    #             xavier_uniform_(self.k_proj_weight)\n",
    "    #             xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "    #         if self.in_proj_bias is not None:\n",
    "    #             constant_(self.in_proj_bias, 0.)\n",
    "    #             constant_(self.out_proj.bias, 0.)\n",
    "    #         if self.bias_k is not None:\n",
    "    #             xavier_normal_(self.bias_k)\n",
    "    #         if self.bias_v is not None:\n",
    "    #             xavier_normal_(self.bias_v)        \n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "class AttentionBlock_new(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, numgroups=8, dim_head = 64, dropout = 0.):  \n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        self.heads = heads\n",
    "        self.attention_norm = nn.GroupNorm(numgroups, dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    " \n",
    "        # nn.init.normal_(self.to_qkv, mean=0., std=np.sqrt(2 / (dim+inner_dim)))\n",
    "        # print(self.to_qkv.weight\n",
    "        # # torch.nn.init.normal_(self.to_qkv, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        in_attn = self.attention_norm(x)\n",
    "        in_attn = x.reshape(b, h * w, c)\n",
    "        qkv = self.to_qkv(in_attn).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "        return out \n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, out_channels, num_heads=4, numgroups=8):\n",
    "        super().__init__()\n",
    "        self.attention_norms = nn.GroupNorm(numgroups, out_channels)\n",
    "        self.attentions = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        # Attention block of Unet\n",
    "        batch_size, channels, h, w = out.shape\n",
    "        in_attn = out.reshape(batch_size, channels, h * w)\n",
    "        in_attn = self.attention_norms(in_attn)\n",
    "        in_attn = in_attn.transpose(1, 2)    #So, I guess: [N, (h*w), C] where (h*w) is the target \"sequence length\", and C is the embedding dimension\n",
    "        out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n",
    "        print('\\nout_attn shape:', out_attn.shape)\n",
    "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "        return out_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "a = 0.02/math.sqrt(2 * (512))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "heads = 4\n",
    "dim_head =128\n",
    "inner_dim = dim_head *  heads\n",
    "numgroups = 8\n",
    "\n",
    "x = torch.randn([2, 256, 32, 32])\n",
    "print('in x shape:', x.shape)\n",
    "\n",
    "b, c, h, w = x.shape\n",
    "norm = nn.GroupNorm(numgroups, dim)\n",
    "in_attn = norm(x)\n",
    "in_attn = x.reshape(b, h * w, c)\n",
    "# in_attn = in_attn.transpose(1, 2)  # reshape to [b, (h*w), c] i.e. [b, seq, emb_dim]\n",
    "print('in_attn shape:', in_attn.shape)\n",
    "\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.normal_(to_qkv.weight.data, mean=0., std=np.sqrt(2 / (dim+inner_dim)))\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.xavier_normal_(to_qkv.weight.data)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "\n",
    "\n",
    "qkv = to_qkv(in_attn)\n",
    "print('out shape:', qkv.shape)\n",
    "\n",
    "qkv = qkv.chunk(3, dim = -1)\n",
    "print('q shape:', qkv[0].shape)\n",
    "\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print('q shape:', q.shape)\n",
    "\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) \n",
    "print('dots shape:', dots.shape)\n",
    "\n",
    "out = torch.matmul(dots, v)\n",
    "print('1 out shape:', out.shape)\n",
    "\n",
    "out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "print('2 out shape:', out.shape)\n",
    "\n",
    "to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "out = to_out(out)\n",
    "print('3 out shape:', out.shape)\n",
    "\n",
    "out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "print('4 out shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "emb_dim = c = 256\n",
    "h = 32\n",
    "w = 32\n",
    "groups = 8\n",
    "heads = 4\n",
    "dim_head = 64 #emb_dim//heads\n",
    "dropout = 0\n",
    "x = torch.randn((b, c, h, w))\n",
    "print('input shape:', x.shape)\n",
    "\n",
    "attn1 = AttentionBlock_new(emb_dim, heads)\n",
    "attn2 = AttentionBlock(c, heads, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1.apply(lambda m: print(type(m).__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = attn1.forward(x)\n",
    "print(out1.shape, ', mean:', torch.mean(out1), ', std:', torch.std(out1))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = attn2.forward(x)\n",
    "print(out2.shape, ', mean:', torch.mean(out2), ', std:', torch.std(out2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "batch = torch.randn([32, 3, 128, 128])\n",
    "make_patches = nn.Conv2d(3, 3, 16, 16, padding=1)\n",
    "\n",
    "patches = make_patches(batch)\n",
    "print('patches shape:', patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "\n",
    "# Verify the mean and the variance: \n",
    "abs(mu - np.mean(s))\n",
    "0.0  # may vary\n",
    "\n",
    "abs(sigma - np.std(s, ddof=1))\n",
    "0.1  # may vary\n",
    "\n",
    "\n",
    "# Display the histogram of the samples, along with the probability density function:\n",
    "count, bins, ignored = plt.hist(s, 30, density=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "from torchvision.transforms.v2 import RandomResizedCrop, RandomRotation, GaussianBlur, RandomErasing\n",
    "\n",
    "from celeba_dataset import CelebA\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Dataset, Dataloader\n",
    "#--------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "image_dir_train = Path('../data/img_align_celeba/img_align_celeba/')\n",
    "\n",
    "img_size = (64,64) \n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "                            RandomHorizontalFlip(p=0.50),\n",
    "                            Resize(img_size, antialias=True)\n",
    "                            ])\n",
    "\n",
    "train_dataset = CelebA(image_dir_train, transform=train_transforms, limit_size=True, size_limit=20)\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=5, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self) : #, mean, std):\n",
    "        pass\n",
    "    def __call__(self, img):\n",
    "        img = (img*127.5) + 127.5\n",
    "        return img\n",
    "    \n",
    "unorm  = UnNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, _  = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(torch.min(images[0]), ', ', torch.max(images[0]))\n",
    "\n",
    "\n",
    "cols = 5\n",
    "rows = 4\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(10, 10))\n",
    "idx = 0\n",
    "for img in (images):  \n",
    "    img = unorm(img).to(torch.uint8).permute(1, 2, 0)\n",
    "    # target = unorm(target).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "    idx += 1\n",
    "    ax = plt.subplot(rows, cols, idx)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    if idx == (cols*rows):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_0, _  = next(iter(train_loader))\n",
    "shape = images_0.shape\n",
    "print(shape)\n",
    "noise = torch.randn(shape[2], shape[3])\n",
    "print(noise.shape)\n",
    "print(images[0:5].shape)\n",
    "\n",
    "imgs_n = lns.add_noise(images[0:1], noise, 50)\n",
    "print(imgs_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 2\n",
    "rows = 1\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(5, 5))\n",
    "idx = 0\n",
    "\n",
    "img   = unorm(images[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "img_n = unorm(imgs_n[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb_dim = 128\n",
    "time_steps = torch.ones((512)) * 999\n",
    "print(time_steps.shape)\n",
    "\n",
    "blah = time_steps[:, None]\n",
    "print(blah.shape)\n",
    "\n",
    "poo = blah.repeat(1, 128//2)\n",
    "print(poo.shape)\n",
    "\n",
    "\n",
    "t_emb = get_time_embedding(time_steps, time_emb_dim)\n",
    "print(t_emb.shape)\n",
    "print(t_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_diffusion import UNet_Diffusion, get_time_embedding\n",
    "from diffusion_lightning import DDPM, EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_location = {'cuda:0':'cuda:1'}\n",
    "# model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_10/checkpoints/epoch=3-step=72936.ckpt',\n",
    "#                                   map_location=map_location) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=  DDPM()\n",
    "trainer = pl.Trainer(accelerator='cpu', devices=1, max_epochs=100) \n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=10,\n",
    "    every_n_epochs=1,\n",
    "    monitor = 'loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "map_location = {'cuda:0':'cuda:1'}\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=\"lightning_logs\", default_hp_metric=False)\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=500,\n",
    "                     logger=logger, log_every_n_steps=1000, callbacks=[checkpoint_callback],\n",
    "                     checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_10/checkpoints/epoch=3-step=72936.ckpt') \n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
