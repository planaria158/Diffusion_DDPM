{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc. code stuff for diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from celeba_dataset import CelebA\n",
    "from unet_diffusion import UNet_Diffusion, get_time_embedding\n",
    "from noise_scheduler import LinearNoiseScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shit1: torch.Size([2, 3, 128, 128]) ,  torch.Size([2, 1])\n",
      "shit2: torch.Size([2, 3, 128, 128]) ,  torch.Size([2])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]          65,792\n",
      "              SiLU-2                  [-1, 256]               0\n",
      "            Linear-3                  [-1, 256]          65,792\n",
      "            Conv2d-4         [-1, 64, 128, 128]           1,792\n",
      "            Conv2d-5         [-1, 64, 128, 128]          36,928\n",
      "         GroupNorm-6         [-1, 64, 128, 128]             128\n",
      "              SiLU-7         [-1, 64, 128, 128]               0\n",
      "              SiLU-8                  [-1, 256]               0\n",
      "            Linear-9                   [-1, 64]          16,448\n",
      "           Conv2d-10         [-1, 64, 128, 128]          36,928\n",
      "        GroupNorm-11         [-1, 64, 128, 128]             128\n",
      "         Identity-12         [-1, 64, 128, 128]               0\n",
      "    ResidualBlock-13         [-1, 64, 128, 128]               0\n",
      "        GroupNorm-14         [-1, 64, 128, 128]             128\n",
      "           Conv2d-15        [-1, 384, 128, 128]          24,576\n",
      "           Conv2d-16         [-1, 64, 128, 128]           8,256\n",
      "        GroupNorm-17         [-1, 64, 128, 128]             128\n",
      "  LinearAttention-18         [-1, 64, 128, 128]               0\n",
      "   AttentionBlock-19         [-1, 64, 128, 128]               0\n",
      "           Conv2d-20         [-1, 64, 128, 128]          36,928\n",
      "        GroupNorm-21         [-1, 64, 128, 128]             128\n",
      "             SiLU-22         [-1, 64, 128, 128]               0\n",
      "             SiLU-23                  [-1, 256]               0\n",
      "           Linear-24                   [-1, 64]          16,448\n",
      "           Conv2d-25         [-1, 64, 128, 128]          36,928\n",
      "        GroupNorm-26         [-1, 64, 128, 128]             128\n",
      "         Identity-27         [-1, 64, 128, 128]               0\n",
      "    ResidualBlock-28         [-1, 64, 128, 128]               0\n",
      "           Conv2d-29         [-1, 64, 128, 128]          36,928\n",
      "           Conv2d-30          [-1, 128, 64, 64]         131,200\n",
      "        DownBlock-31  [[-1, 128, 64, 64], [-1, 64, 128, 128]]               0\n",
      "           Conv2d-32          [-1, 128, 64, 64]         147,584\n",
      "        GroupNorm-33          [-1, 128, 64, 64]             256\n",
      "             SiLU-34          [-1, 128, 64, 64]               0\n",
      "             SiLU-35                  [-1, 256]               0\n",
      "           Linear-36                  [-1, 128]          32,896\n",
      "           Conv2d-37          [-1, 128, 64, 64]         147,584\n",
      "        GroupNorm-38          [-1, 128, 64, 64]             256\n",
      "         Identity-39          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-40          [-1, 128, 64, 64]               0\n",
      "        GroupNorm-41          [-1, 128, 64, 64]             256\n",
      "           Conv2d-42          [-1, 384, 64, 64]          49,152\n",
      "           Conv2d-43          [-1, 128, 64, 64]          16,512\n",
      "        GroupNorm-44          [-1, 128, 64, 64]             256\n",
      "  LinearAttention-45          [-1, 128, 64, 64]               0\n",
      "   AttentionBlock-46          [-1, 128, 64, 64]               0\n",
      "           Conv2d-47          [-1, 128, 64, 64]         147,584\n",
      "        GroupNorm-48          [-1, 128, 64, 64]             256\n",
      "             SiLU-49          [-1, 128, 64, 64]               0\n",
      "             SiLU-50                  [-1, 256]               0\n",
      "           Linear-51                  [-1, 128]          32,896\n",
      "           Conv2d-52          [-1, 128, 64, 64]         147,584\n",
      "        GroupNorm-53          [-1, 128, 64, 64]             256\n",
      "         Identity-54          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-55          [-1, 128, 64, 64]               0\n",
      "           Conv2d-56          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-57          [-1, 256, 32, 32]         524,544\n",
      "        DownBlock-58  [[-1, 256, 32, 32], [-1, 128, 64, 64]]               0\n",
      "           Conv2d-59          [-1, 256, 32, 32]         590,080\n",
      "        GroupNorm-60          [-1, 256, 32, 32]             512\n",
      "             SiLU-61          [-1, 256, 32, 32]               0\n",
      "             SiLU-62                  [-1, 256]               0\n",
      "           Linear-63                  [-1, 256]          65,792\n",
      "           Conv2d-64          [-1, 256, 32, 32]         590,080\n",
      "        GroupNorm-65          [-1, 256, 32, 32]             512\n",
      "         Identity-66          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-67          [-1, 256, 32, 32]               0\n",
      "        GroupNorm-68          [-1, 256, 32, 32]             512\n",
      "           Conv2d-69          [-1, 384, 32, 32]          98,304\n",
      "           Conv2d-70          [-1, 256, 32, 32]          33,024\n",
      "        GroupNorm-71          [-1, 256, 32, 32]             512\n",
      "  LinearAttention-72          [-1, 256, 32, 32]               0\n",
      "   AttentionBlock-73          [-1, 256, 32, 32]               0\n",
      "           Conv2d-74          [-1, 256, 32, 32]         590,080\n",
      "        GroupNorm-75          [-1, 256, 32, 32]             512\n",
      "             SiLU-76          [-1, 256, 32, 32]               0\n",
      "             SiLU-77                  [-1, 256]               0\n",
      "           Linear-78                  [-1, 256]          65,792\n",
      "           Conv2d-79          [-1, 256, 32, 32]         590,080\n",
      "        GroupNorm-80          [-1, 256, 32, 32]             512\n",
      "         Identity-81          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-82          [-1, 256, 32, 32]               0\n",
      "           Conv2d-83          [-1, 256, 32, 32]         590,080\n",
      "           Conv2d-84          [-1, 512, 16, 16]       2,097,664\n",
      "        DownBlock-85  [[-1, 512, 16, 16], [-1, 256, 32, 32]]               0\n",
      "           Conv2d-86          [-1, 512, 16, 16]       2,359,808\n",
      "        GroupNorm-87          [-1, 512, 16, 16]           1,024\n",
      "             SiLU-88          [-1, 512, 16, 16]               0\n",
      "             SiLU-89                  [-1, 256]               0\n",
      "           Linear-90                  [-1, 512]         131,584\n",
      "           Conv2d-91          [-1, 512, 16, 16]       2,359,808\n",
      "        GroupNorm-92          [-1, 512, 16, 16]           1,024\n",
      "         Identity-93          [-1, 512, 16, 16]               0\n",
      "    ResidualBlock-94          [-1, 512, 16, 16]               0\n",
      "        GroupNorm-95          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-96          [-1, 384, 16, 16]         196,608\n",
      "           Conv2d-97          [-1, 512, 16, 16]          66,048\n",
      "        GroupNorm-98          [-1, 512, 16, 16]           1,024\n",
      "  LinearAttention-99          [-1, 512, 16, 16]               0\n",
      "  AttentionBlock-100          [-1, 512, 16, 16]               0\n",
      "          Conv2d-101          [-1, 512, 16, 16]       2,359,808\n",
      "       GroupNorm-102          [-1, 512, 16, 16]           1,024\n",
      "            SiLU-103          [-1, 512, 16, 16]               0\n",
      "            SiLU-104                  [-1, 256]               0\n",
      "          Linear-105                  [-1, 512]         131,584\n",
      "          Conv2d-106          [-1, 512, 16, 16]       2,359,808\n",
      "       GroupNorm-107          [-1, 512, 16, 16]           1,024\n",
      "        Identity-108          [-1, 512, 16, 16]               0\n",
      "   ResidualBlock-109          [-1, 512, 16, 16]               0\n",
      "          Conv2d-110          [-1, 512, 16, 16]       2,359,808\n",
      "          Conv2d-111           [-1, 1024, 8, 8]       8,389,632\n",
      "       DownBlock-112  [[-1, 1024, 8, 8], [-1, 512, 16, 16]]               0\n",
      "          Conv2d-113           [-1, 1024, 8, 8]       9,438,208\n",
      "       GroupNorm-114           [-1, 1024, 8, 8]           2,048\n",
      "            SiLU-115           [-1, 1024, 8, 8]               0\n",
      "            SiLU-116                  [-1, 256]               0\n",
      "          Linear-117                 [-1, 1024]         263,168\n",
      "          Conv2d-118           [-1, 1024, 8, 8]       9,438,208\n",
      "       GroupNorm-119           [-1, 1024, 8, 8]           2,048\n",
      "        Identity-120           [-1, 1024, 8, 8]               0\n",
      "   ResidualBlock-121           [-1, 1024, 8, 8]               0\n",
      "       GroupNorm-122           [-1, 1024, 8, 8]           2,048\n",
      "          Conv2d-123            [-1, 384, 8, 8]         393,216\n",
      "          Conv2d-124           [-1, 1024, 8, 8]         132,096\n",
      "       Attention-125           [-1, 1024, 8, 8]               0\n",
      "  AttentionBlock-126           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-127           [-1, 1024, 8, 8]       9,438,208\n",
      "       GroupNorm-128           [-1, 1024, 8, 8]           2,048\n",
      "            SiLU-129           [-1, 1024, 8, 8]               0\n",
      "            SiLU-130                  [-1, 256]               0\n",
      "          Linear-131                 [-1, 1024]         263,168\n",
      "          Conv2d-132           [-1, 1024, 8, 8]       9,438,208\n",
      "       GroupNorm-133           [-1, 1024, 8, 8]           2,048\n",
      "        Identity-134           [-1, 1024, 8, 8]               0\n",
      "   ResidualBlock-135           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-136           [-1, 1024, 8, 8]       9,438,208\n",
      "        MidBlock-137           [-1, 1024, 8, 8]               0\n",
      " ConvTranspose2d-138         [-1, 1024, 16, 16]      16,778,240\n",
      "          Conv2d-139         [-1, 1536, 16, 16]      21,235,200\n",
      "       GroupNorm-140         [-1, 1536, 16, 16]           3,072\n",
      "            SiLU-141         [-1, 1536, 16, 16]               0\n",
      "            SiLU-142                  [-1, 256]               0\n",
      "          Linear-143                 [-1, 1536]         394,752\n",
      "          Conv2d-144         [-1, 1536, 16, 16]      21,235,200\n",
      "       GroupNorm-145         [-1, 1536, 16, 16]           3,072\n",
      "        Identity-146         [-1, 1536, 16, 16]               0\n",
      "   ResidualBlock-147         [-1, 1536, 16, 16]               0\n",
      "       GroupNorm-148         [-1, 1536, 16, 16]           3,072\n",
      "          Conv2d-149          [-1, 384, 16, 16]         589,824\n",
      "          Conv2d-150         [-1, 1536, 16, 16]         198,144\n",
      "       GroupNorm-151         [-1, 1536, 16, 16]           3,072\n",
      " LinearAttention-152         [-1, 1536, 16, 16]               0\n",
      "  AttentionBlock-153         [-1, 1536, 16, 16]               0\n",
      "          Conv2d-154         [-1, 1536, 16, 16]      21,235,200\n",
      "       GroupNorm-155         [-1, 1536, 16, 16]           3,072\n",
      "            SiLU-156         [-1, 1536, 16, 16]               0\n",
      "            SiLU-157                  [-1, 256]               0\n",
      "          Linear-158                 [-1, 1536]         394,752\n",
      "          Conv2d-159          [-1, 512, 16, 16]       7,078,400\n",
      "       GroupNorm-160          [-1, 512, 16, 16]           1,024\n",
      "          Conv2d-161          [-1, 512, 16, 16]         786,944\n",
      "   ResidualBlock-162          [-1, 512, 16, 16]               0\n",
      "          Conv2d-163          [-1, 512, 16, 16]       2,359,808\n",
      "         UpBlock-164          [-1, 512, 16, 16]               0\n",
      " ConvTranspose2d-165          [-1, 512, 32, 32]       4,194,816\n",
      "          Conv2d-166          [-1, 768, 32, 32]       5,309,184\n",
      "       GroupNorm-167          [-1, 768, 32, 32]           1,536\n",
      "            SiLU-168          [-1, 768, 32, 32]               0\n",
      "            SiLU-169                  [-1, 256]               0\n",
      "          Linear-170                  [-1, 768]         197,376\n",
      "          Conv2d-171          [-1, 768, 32, 32]       5,309,184\n",
      "       GroupNorm-172          [-1, 768, 32, 32]           1,536\n",
      "        Identity-173          [-1, 768, 32, 32]               0\n",
      "   ResidualBlock-174          [-1, 768, 32, 32]               0\n",
      "       GroupNorm-175          [-1, 768, 32, 32]           1,536\n",
      "          Conv2d-176          [-1, 384, 32, 32]         294,912\n",
      "          Conv2d-177          [-1, 768, 32, 32]          99,072\n",
      "       GroupNorm-178          [-1, 768, 32, 32]           1,536\n",
      " LinearAttention-179          [-1, 768, 32, 32]               0\n",
      "  AttentionBlock-180          [-1, 768, 32, 32]               0\n",
      "          Conv2d-181          [-1, 768, 32, 32]       5,309,184\n",
      "       GroupNorm-182          [-1, 768, 32, 32]           1,536\n",
      "            SiLU-183          [-1, 768, 32, 32]               0\n",
      "            SiLU-184                  [-1, 256]               0\n",
      "          Linear-185                  [-1, 768]         197,376\n",
      "          Conv2d-186          [-1, 256, 32, 32]       1,769,728\n",
      "       GroupNorm-187          [-1, 256, 32, 32]             512\n",
      "          Conv2d-188          [-1, 256, 32, 32]         196,864\n",
      "   ResidualBlock-189          [-1, 256, 32, 32]               0\n",
      "          Conv2d-190          [-1, 256, 32, 32]         590,080\n",
      "         UpBlock-191          [-1, 256, 32, 32]               0\n",
      " ConvTranspose2d-192          [-1, 256, 64, 64]       1,048,832\n",
      "          Conv2d-193          [-1, 384, 64, 64]       1,327,488\n",
      "       GroupNorm-194          [-1, 384, 64, 64]             768\n",
      "            SiLU-195          [-1, 384, 64, 64]               0\n",
      "            SiLU-196                  [-1, 256]               0\n",
      "          Linear-197                  [-1, 384]          98,688\n",
      "          Conv2d-198          [-1, 384, 64, 64]       1,327,488\n",
      "       GroupNorm-199          [-1, 384, 64, 64]             768\n",
      "        Identity-200          [-1, 384, 64, 64]               0\n",
      "   ResidualBlock-201          [-1, 384, 64, 64]               0\n",
      "       GroupNorm-202          [-1, 384, 64, 64]             768\n",
      "          Conv2d-203          [-1, 384, 64, 64]         147,456\n",
      "          Conv2d-204          [-1, 384, 64, 64]          49,536\n",
      "       GroupNorm-205          [-1, 384, 64, 64]             768\n",
      " LinearAttention-206          [-1, 384, 64, 64]               0\n",
      "  AttentionBlock-207          [-1, 384, 64, 64]               0\n",
      "          Conv2d-208          [-1, 384, 64, 64]       1,327,488\n",
      "       GroupNorm-209          [-1, 384, 64, 64]             768\n",
      "            SiLU-210          [-1, 384, 64, 64]               0\n",
      "            SiLU-211                  [-1, 256]               0\n",
      "          Linear-212                  [-1, 384]          98,688\n",
      "          Conv2d-213          [-1, 128, 64, 64]         442,496\n",
      "       GroupNorm-214          [-1, 128, 64, 64]             256\n",
      "          Conv2d-215          [-1, 128, 64, 64]          49,280\n",
      "   ResidualBlock-216          [-1, 128, 64, 64]               0\n",
      "          Conv2d-217          [-1, 128, 64, 64]         147,584\n",
      "         UpBlock-218          [-1, 128, 64, 64]               0\n",
      " ConvTranspose2d-219        [-1, 128, 128, 128]         262,272\n",
      "          Conv2d-220        [-1, 192, 128, 128]         331,968\n",
      "       GroupNorm-221        [-1, 192, 128, 128]             384\n",
      "            SiLU-222        [-1, 192, 128, 128]               0\n",
      "            SiLU-223                  [-1, 256]               0\n",
      "          Linear-224                  [-1, 192]          49,344\n",
      "          Conv2d-225        [-1, 192, 128, 128]         331,968\n",
      "       GroupNorm-226        [-1, 192, 128, 128]             384\n",
      "        Identity-227        [-1, 192, 128, 128]               0\n",
      "   ResidualBlock-228        [-1, 192, 128, 128]               0\n",
      "       GroupNorm-229        [-1, 192, 128, 128]             384\n",
      "          Conv2d-230        [-1, 384, 128, 128]          73,728\n",
      "          Conv2d-231        [-1, 192, 128, 128]          24,768\n",
      "       GroupNorm-232        [-1, 192, 128, 128]             384\n",
      " LinearAttention-233        [-1, 192, 128, 128]               0\n",
      "  AttentionBlock-234        [-1, 192, 128, 128]               0\n",
      "          Conv2d-235        [-1, 192, 128, 128]         331,968\n",
      "       GroupNorm-236        [-1, 192, 128, 128]             384\n",
      "            SiLU-237        [-1, 192, 128, 128]               0\n",
      "            SiLU-238                  [-1, 256]               0\n",
      "          Linear-239                  [-1, 192]          49,344\n",
      "          Conv2d-240         [-1, 64, 128, 128]         110,656\n",
      "       GroupNorm-241         [-1, 64, 128, 128]             128\n",
      "          Conv2d-242         [-1, 64, 128, 128]          12,352\n",
      "   ResidualBlock-243         [-1, 64, 128, 128]               0\n",
      "          Conv2d-244         [-1, 64, 128, 128]          36,928\n",
      "         UpBlock-245         [-1, 64, 128, 128]               0\n",
      "       GroupNorm-246         [-1, 64, 128, 128]             128\n",
      "          Conv2d-247         [-1, 32, 128, 128]          18,464\n",
      "       GroupNorm-248         [-1, 32, 128, 128]              64\n",
      "          Conv2d-249          [-1, 3, 128, 128]              99\n",
      "  UNet_Diffusion-250          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 199,682,819\n",
      "Trainable params: 199,682,819\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 5569294.95\n",
      "Params size (MB): 761.73\n",
      "Estimated Total Size (MB): 5570056.50\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(199682819), tensor(199682819))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "from diffusion_lightning import DDPM\n",
    "import yaml\n",
    "\n",
    "from utils.torchsummary import summary\n",
    "\n",
    "with open('./config/default.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "diffusion_config = config['diffusion_params']\n",
    "dataset_config = config['dataset_params']\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']\n",
    "ddpm = DDPM(model_config, diffusion_config)\n",
    "\n",
    "model = ddpm.model\n",
    "# print(model)\n",
    "\n",
    "summary(model, input_size=[(3, 128, 128), (1,)], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.view(-1, 32*7*7)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# Print the PyTorch model parameters \n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.torchsummary as torchsummary\n",
    "\n",
    "torchsummary.summary(model, input_size=(3, 224, 224), device='cpu')   #torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import mlflow.pytorch\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "# mlflow.set_experiment(experiment_name='mlflow_autolog_testing')\n",
    "\n",
    "class MNISTModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "        self.accuracy = Accuracy(\"multiclass\", num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        acc = self.accuracy(pred, y)\n",
    "\n",
    "        # PyTorch `self.log` will be automatically captured by MLflow.\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        self.log(\"acc\", acc, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "\n",
    "\n",
    "def print_auto_logged_info(r):\n",
    "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
    "    print(f\"run_id: {r.info.run_id}\")\n",
    "    print(f\"artifacts: {artifacts}\")\n",
    "    print(f\"params: {r.data.params}\")\n",
    "    print(f\"metrics: {r.data.metrics}\")\n",
    "    print(f\"tags: {tags}\")\n",
    "\n",
    "\n",
    "# Initialize our model.\n",
    "mnist_model = MNISTModel()\n",
    "\n",
    "# Load MNIST dataset.\n",
    "train_ds = MNIST(\n",
    "    os.getcwd(), train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "# Only take a subset of the data for faster training.\n",
    "indices = torch.arange(32)\n",
    "train_ds = Subset(train_ds, indices)\n",
    "train_loader = DataLoader(train_ds, batch_size=8)\n",
    "\n",
    "# Initialize a trainer.\n",
    "trainer = L.Trainer(max_epochs=1, accelerator=\"cpu\")\n",
    "\n",
    "# Auto log all MLflow entities\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "print('tracking URI:', mlflow.get_tracking_uri())\n",
    "\n",
    "# Train the model.\n",
    "with mlflow.start_run() as run:\n",
    "    trainer.fit(mnist_model, train_loader)\n",
    "\n",
    "# Fetch the auto logged parameters and metrics.\n",
    "# print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(reversed(range(0, 10, 2))):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Make video frame from the image predictions created during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import moviepy.video.io.ImageSequenceClip\n",
    "\n",
    "# output_format could be .mp4, .avi, .mov, etc\n",
    "def images_to_video(image_folder_path:str, fps, extension:str, video_name:str, output_format:str):    \n",
    "    images = [image_folder_path+'/'+img for img in os.listdir(image_folder_path) if img.endswith(extension)]\n",
    "    images.sort()\n",
    "    movie_clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(images, fps)\n",
    "    movie_clip.write_videofile(video_name+output_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "frames = glob.glob('./default/samples/*.png')\n",
    "frames.sort() #key=os.path.getctime)\n",
    "print(len(frames))\n",
    "print(frames[0])\n",
    "print(frames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_video('./default/samples/', 5, '.png', 'train_video', '.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "# Calculate the final mean loss over the timesteps used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO\n",
    "\n",
    "img_size = ?? (64,64) #(128,128) \n",
    "time_emb_dim = 256\n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "lns = LinearNoiseScheduler(num_timesteps, beta_start, beta_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duh =  torch.zeros((10))\n",
    "count = torch.zeros((10))\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "input = torch.randn(5)\n",
    "target = torch.randn(5)\n",
    "tstep = torch.randint(0, 10, (input.shape)) \n",
    "print('tstep:', tstep)\n",
    "\n",
    "output = loss(input, target)\n",
    "a = (input-target)**2\n",
    "\n",
    "duh[tstep] += a\n",
    "print('duh:', duh)\n",
    "\n",
    "print (output.shape, ', ', output.item())\n",
    "print(a.shape, ', ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Image predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If run on a Mac M1, use [MPS acceleration](https://pytorch.org/docs/stable/notes/mps.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "\n",
    "import torchvision\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "from torchvision.utils import make_grid\n",
    "from unet_diffusion import UNet_Diffusion\n",
    "\n",
    "from diffusion_lightning import DDPM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "img_size = (128,128)\n",
    "time_emb_dim = 256\n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "lns = LinearNoiseScheduler(num_timesteps, beta_start, beta_end)\n",
    "\n",
    "num_samples = 16\n",
    "num_grid_rows = 4\n",
    "im_channels = 3\n",
    "im_size = img_size[0]\n",
    "task_name = 'default'\n",
    "\n",
    "device = 'cpu' #torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def sample(model, scheduler, delt):\n",
    "    \"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    We save the x0 predictions\n",
    "    \"\"\"\n",
    "\n",
    "    xt = torch.randn((num_samples, im_channels, im_size, im_size)).to(device)\n",
    "\n",
    "    for i in tqdm(reversed(range(num_timesteps))):\n",
    "    # for i in reversed(range(0, num_timesteps, delt)):\n",
    "        # print(i)\n",
    "        # Get prediction of noise\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep_ddpm(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "        # xt = scheduler.sample_prev_timestep_ddim(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "        \n",
    "        # Save x0 every 200th time.\n",
    "        if i % 200 == 0 or (i == num_timesteps-1):\n",
    "            print('i:', i, ', Saving image')\n",
    "            ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "            ims = (ims + 1) / 2\n",
    "            grid = make_grid(ims, nrow=num_grid_rows)\n",
    "            img = torchvision.transforms.ToPILImage()(grid)\n",
    "            if not os.path.exists(os.path.join(task_name, 'samples_test')):\n",
    "                os.makedirs(os.path.join(task_name, 'samples_test'))\n",
    "            print('saving:', os.path.join(task_name, 'samples_test', 'x0_{}.png'.format(i)))\n",
    "            img.save(os.path.join(task_name, 'samples_test', 'x0_{}.png'.format(i)))\n",
    "            img.close()\n",
    "\n",
    "\n",
    "def infer():\n",
    "    model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_30/checkpoints/epoch=92-step=529914.ckpt',\n",
    "                                      map_location=torch.device(\"cpu\"))\n",
    "    model.ema_model = None # dump the extra EMA model used at train-time only\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    print('Model has:', int(total_params//1e6), 'M parameters')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=num_timesteps,\n",
    "                                     beta_start=beta_start,\n",
    "                                     beta_end=beta_end)\n",
    "    with torch.no_grad():\n",
    "        delt = 2 # every 10 t steps for ddim\n",
    "        sample(model.model, scheduler, delt)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Run the inference\n",
    "#----------------------------------------------------\n",
    "infer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_lightning import DDPM\n",
    "\n",
    "model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_30_128x128/checkpoints/epoch=99-step=569800.ckpt',\n",
    "                                    map_location=torch.device(\"cpu\"))\n",
    "model.ema_model = None # dump the extra EMA model used at train-time only\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print('Model has:', int(total_params//1e6), 'M parameters')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(model.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Generate a grid of diffusion images\n",
    "    def _sample(self):\n",
    "        # use self.ema_model since it's already set to eval, grad_not_required\n",
    "        device = self.device\n",
    "        task_name = self.task_name\n",
    "\n",
    "        xt = torch.randn((self.num_samples, 3, self.img_size[0], self.img_size[1])).to(device)\n",
    "\n",
    "        for i in tqdm(reversed(range(self.num_timesteps))):\n",
    "            # Get prediction of noise\n",
    "            noise_pred = self.ema_model(xt, torch.as_tensor(i).unsqueeze(0).to(device))            \n",
    "            # Use scheduler to get x0 and xt-1\n",
    "            xt, x0_pred = self.scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "            \n",
    "        # Save final predicted image Xo to file.\n",
    "        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "        ims = (ims + 1) / 2\n",
    "        grid = make_grid(ims, nrow=self.num_grid_rows)\n",
    "        img = torchvision.transforms.ToPILImage()(grid)\n",
    "        out_path = os.path.join(task_name, 'samples') \n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        img.save(os.path.join(out_path, 'x0_epoch_{}.png'.format(self.current_epoch)))\n",
    "        img.close()\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Perception-prioritized Weighting during training.\n",
    "https://arxiv.org/pdf/2204.00227.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perception-prioritized weighting during training \n",
    "# https://arxiv.org/pdf/2204.00227.pdf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha_cum_prod = lns.alpha_cum_prod \n",
    "betas = lns.betas\n",
    "alphas = lns.alphas\n",
    "\n",
    "lambda_t = ((1 - betas) * (1 - alpha_cum_prod))/betas\n",
    "snr =  (1.0/(1.0 - alpha_cum_prod)) - 1  #alpha_cum_prod/(1.0 - alpha_cum_prod) \n",
    "t_steps = torch.linspace(0, 1000, 1000, dtype=torch.int)\n",
    "assert snr.shape == t_steps.shape\n",
    "\n",
    "p2_k = 1\n",
    "p2_gamma = 0.5  \n",
    "weights_0p5 = (lambda_t/(p2_k + snr)**p2_gamma)\n",
    "# plot normalized weights.\n",
    "# weights_0p5 /= (torch.sum(torch.abs(weights_0p5)))\n",
    "# print(torch.sum(weights_0p5))\n",
    "\n",
    "p2_gamma = 1.0  \n",
    "weights_1p0 = (lambda_t/(p2_k + snr)**p2_gamma)\n",
    "# weights_1p0 /= (torch.sum(torch.abs(weights_1p0)))\n",
    "# print(torch.sum(weights_1p0))\n",
    "\n",
    "weights_baseline = lambda_t\n",
    "# weights_baseline /= (torch.sum(torch.abs(weights_baseline)))\n",
    "# print(torch.sum(weights_baseline))\n",
    "\n",
    "rows = 1\n",
    "cols = 3\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "ax = plt.subplot(rows, cols, 1)\n",
    "ax.plot(t_steps, snr, linewidth=2.0)\n",
    "plt.title('Signal-to-noise ratio (SNR)')\n",
    "plt.xlabel('time step')\n",
    "plt.ylabel('SNR')\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-9, 1e5)\n",
    "\n",
    "ax = plt.subplot(rows, cols, 2)\n",
    "ax.plot(snr, weights_baseline, linewidth=2.0, color=\"blue\")\n",
    "ax.plot(snr, weights_0p5, linewidth=2.0, color=\"orange\")\n",
    "ax.plot(snr, weights_1p0, linewidth=2.0, color=\"green\")\n",
    "plt.title('Linear Schedule')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('weights')\n",
    "plt.xscale(\"log\")\n",
    "# plt.xlim(1e-8, 50)\n",
    "\n",
    "# ax = plt.subplot(rows, cols, 3)\n",
    "# ax.plot(t_steps, torch.log10(snr)/weights_baseline, linewidth=2.0, color=\"blue\")\n",
    "# ax.plot(t_steps, torch.log10(snr)/weights_0p5, linewidth=2.0, color=\"orange\")\n",
    "# ax.plot(t_steps, torch.log10(snr)/weights_1p0, linewidth=2.0, color=\"green\")\n",
    "# plt.title('')\n",
    "# plt.ylabel('SNR / weights')\n",
    "# plt.xlabel('time steps')\n",
    "# plt.xscale(\"log\")\n",
    "# plt.xlim(1e-8, 1e4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the PP weights stuff added to noise_scheduler class.\n",
    "\n",
    "# Perception-prioritized weighting during training \n",
    "# https://arxiv.org/pdf/2204.00227.pdf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_steps = torch.linspace(0, 999, 1000, dtype=torch.int)\n",
    "weights = lns.get_pp_weights(t_steps)\n",
    "for i in range(3):\n",
    "    weights = weights.squeeze()\n",
    "\n",
    "snr = lns.snr[t_steps]\n",
    "print('weights shape:', weights.shape)\n",
    "print('snr shape:', snr.shape)\n",
    "\n",
    "# weights_1p0 /= (torch.sum(torch.abs(weights_1p0)))\n",
    "# print(torch.sum(weights_1p0))\n",
    "\n",
    "rows = 1\n",
    "cols = 1\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "ax = plt.subplot(rows, cols, 1)\n",
    "ax.plot(snr, weights, linewidth=2.0, color=\"green\")\n",
    "plt.title('Linear Schedule')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('weights')\n",
    "plt.xscale(\"log\")\n",
    "# plt.xlim(1e-8, 50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "# Some debugging of multi-headed self-attention code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat, rearrange\n",
    "patch_dim = 16\n",
    "in_channels = 64\n",
    "img_shape = 64\n",
    "\n",
    "token_dim = patch_dim * patch_dim * in_channels  # length of linearized rgb patchs\n",
    "# dim = dim # The embedding dimension for the Encoder\n",
    "# num_patches = ((img_shape[0]//patch_dim) * (img_shape[0]//patch_dim))\n",
    "\n",
    "# chunk1 = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_dim, p2 = patch_dim)\n",
    "chunk1 = Rearrange('b c (h p1) (w p2) -> b (p1 p2 c) (h w)', p1 = patch_dim, p2 = patch_dim)\n",
    "# chunk1 = Rearrange('b c (h p1) (w p2) -> b (h w) c (p1 p2)', p1 = patch_dim, p2 = patch_dim)\n",
    "chunk2 = nn.Conv2d(in_channels, in_channels, (8,8), 8)\n",
    "\n",
    "x = torch.randn([1, in_channels, img_shape, img_shape])\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "x1 = chunk1(x)\n",
    "print('chunk1 x1 shape:', x1.shape)\n",
    "\n",
    "x1 = chunk2(x)\n",
    "print('chunk2 x1 shape:', x1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "heads = 4\n",
    "dim_head =128\n",
    "inner_dim = dim_head *  heads\n",
    "numgroups = 8\n",
    "\n",
    "x = torch.randn([2, 256, 32, 32])\n",
    "print('in x shape:', x.shape)\n",
    "\n",
    "b, c, h, w = x.shape\n",
    "norm = nn.GroupNorm(numgroups, dim)\n",
    "in_attn = norm(x)\n",
    "in_attn = x.reshape(b, h * w, c)\n",
    "# in_attn = in_attn.transpose(1, 2)  # reshape to [b, (h*w), c] i.e. [b, seq, emb_dim]\n",
    "print('in_attn shape:', in_attn.shape)\n",
    "\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.normal_(to_qkv.weight.data, mean=0., std=np.sqrt(2 / (dim+inner_dim)))\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.xavier_normal_(to_qkv.weight.data)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "\n",
    "\n",
    "qkv = to_qkv(in_attn)\n",
    "print('out shape:', qkv.shape)\n",
    "\n",
    "qkv = qkv.chunk(3, dim = -1)\n",
    "print('q shape:', qkv[0].shape)\n",
    "\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print('q shape:', q.shape)\n",
    "\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) \n",
    "print('dots shape:', dots.shape)\n",
    "\n",
    "out = torch.matmul(dots, v)\n",
    "print('1 out shape:', out.shape)\n",
    "\n",
    "out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "print('2 out shape:', out.shape)\n",
    "\n",
    "to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "out = to_out(out)\n",
    "print('3 out shape:', out.shape)\n",
    "\n",
    "out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "print('4 out shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "\n",
    "# Verify the mean and the variance: \n",
    "abs(mu - np.mean(s))\n",
    "0.0  # may vary\n",
    "\n",
    "abs(sigma - np.std(s, ddof=1))\n",
    "0.1  # may vary\n",
    "\n",
    "\n",
    "# Display the histogram of the samples, along with the probability density function:\n",
    "count, bins, ignored = plt.hist(s, 30, density=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# FID score stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload\n",
    "import os\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "\n",
    "from celeba_dataset import CelebA\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Dataset, Dataloader\n",
    "#--------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "image_dir_train = Path('../data/img_align_celeba/img_align_celeba/train/')\n",
    "\n",
    "img_size = (64,64) \n",
    "batch_size = 50\n",
    "\n",
    "# train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "#                             RandomHorizontalFlip(p=0.50),\n",
    "#                             Resize(img_size, antialias=True)\n",
    "#                             ])\n",
    "\n",
    "train_dataset = CelebA(image_dir_train, transform=None, limit_size=False, size_limit=-1)\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = False, num_workers=5, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(feature=2048) #.set_dtype(torch.float64)\n",
    "# generate two slightly overlapping image intensity distributions\n",
    "\n",
    "diter = iter(train_loader)\n",
    "fid_scores = []\n",
    "\n",
    "for i in range(1000) :\n",
    "    if i % 10 == 0:\n",
    "        print('batch:', i+1)\n",
    "\n",
    "    imgs_dist1, _  = next(diter)\n",
    "    imgs_dist1 = imgs_dist1.type(torch.uint8)\n",
    "\n",
    "    imgs_dist2, _  = next(diter)\n",
    "    imgs_dist2 = imgs_dist2.type(torch.uint8)\n",
    "\n",
    "    fid.update(imgs_dist1, real=True)\n",
    "    fid.update(imgs_dist2, real=False)\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        fid_scores.append(fid.compute().item())\n",
    "        print('batch:', i+1, ', fid:', fid_scores[-1])\n",
    "\n",
    "print('Computing FID score')\n",
    "val = fid.compute()\n",
    "print(val)\n",
    "# fig_, ax_ = fid.plot()\n",
    "\n",
    "print(fid_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images 1000, FID: 27.318\n",
    "# images 5000, FID: 6.4046\n",
    "# images 10000, FID: 3.2235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(feature=2048).set_dtype(torch.float64)\n",
    "# generate two slightly overlapping image intensity distributions\n",
    "imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "fid.update(imgs_dist1, real=True)\n",
    "fid.update(imgs_dist2, real=False)\n",
    "val = fid.compute()\n",
    "print(val)\n",
    "fig_, ax_ = fid.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# Some image plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self) : #, mean, std):\n",
    "        pass\n",
    "    def __call__(self, img):\n",
    "        img = (img*127.5) + 127.5\n",
    "        return img\n",
    "    \n",
    "unorm  = UnNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, _  = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(torch.min(images[0]), ', ', torch.max(images[0]))\n",
    "\n",
    "\n",
    "cols = 5\n",
    "rows = 4\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(10, 10))\n",
    "idx = 0\n",
    "for img in (images):  \n",
    "    img = unorm(img).to(torch.uint8).permute(1, 2, 0)\n",
    "    # target = unorm(target).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "    idx += 1\n",
    "    ax = plt.subplot(rows, cols, idx)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    if idx == (cols*rows):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_0, _  = next(iter(train_loader))\n",
    "shape = images_0.shape\n",
    "print(shape)\n",
    "noise = torch.randn(shape[2], shape[3])\n",
    "print(noise.shape)\n",
    "print(images[0:5].shape)\n",
    "\n",
    "imgs_n = lns.add_noise(images[0:1], noise, 50)\n",
    "print(imgs_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 2\n",
    "rows = 1\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(5, 5))\n",
    "idx = 0\n",
    "\n",
    "img   = unorm(images[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "img_n = unorm(imgs_n[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb_dim = 128\n",
    "time_steps = torch.ones((512)) * 999\n",
    "print(time_steps.shape)\n",
    "\n",
    "blah = time_steps[:, None]\n",
    "print(blah.shape)\n",
    "\n",
    "poo = blah.repeat(1, 128//2)\n",
    "print(poo.shape)\n",
    "\n",
    "\n",
    "t_emb = get_time_embedding(time_steps, time_emb_dim)\n",
    "print(t_emb.shape)\n",
    "print(t_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# Misc debugging code for the AttentionBlock (pytorch's vs. the hand-crafted version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn\n",
    "from math import prod\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dim_head=64, numgroups=8, dropout=0.):  \n",
    "        super().__init__()        \n",
    "        inner_dim = dim_head * num_heads\n",
    "        # dim_head = dim // num_heads\n",
    "        # inner_dim = dim \n",
    "        print('dim:', dim, ', inner dim:', inner_dim)\n",
    "\n",
    "        project_out = not (num_heads == 1 and dim_head == dim)\n",
    "        self.heads = num_heads\n",
    "        self.attention_norm = nn.GroupNorm(numgroups, dim)\n",
    "        self.scale = float(dim_head) ** -0.5\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        print('[b, c, h, w]:', b, c, h, w)\n",
    "        in_attn = x.reshape(b, c, h * w)\n",
    "        print('1. in_attn shape:', in_attn.shape)\n",
    "        # GroupNorm applies only to the c channels, so the dimensions of the tensor \n",
    "        # after that is probably not important either way\n",
    "        in_attn = self.attention_norm(in_attn) \n",
    "\n",
    "        # in_attn = rearrange(in_attn, 'b c (h p1 w p2) -> b (p1 p2 c) (h w)', h=4, p1 = 16, w=4, p2 = 16) # shit, nah! doesn\n",
    "        in_attn = in_attn.transpose(1,2)\n",
    "        \n",
    "        print('2. in_attn shape:', in_attn.shape)\n",
    "        qkv = self.to_qkv(in_attn).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        print('q shape:', q.shape, ', k shape:', k.shape, ', v shape:', v.shape)\n",
    "        print('q Mb:', (prod(q.shape)*8)/1e6)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        print('dots shape:', dots.shape, ', size Mb:', (prod(dots.shape)*8)/1e6)\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        out = torch.matmul(attn, v)\n",
    "        print('out shape:', out.shape, ', size Mb:', (prod(out.shape)*8)/1e6)\n",
    "        # # print('1. out shape:', out.shape)\n",
    "        # out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        # # print('2. out shape:', out.shape)\n",
    "        # out = self.to_out(out)\n",
    "        # # print('3. out shape:', out.shape)\n",
    "        # out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "        # # print('4. out shape:', out.shape)\n",
    "        return None #out     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import collections\n",
    "# class AttentionBlock(nn.Module):\n",
    "#     def __init__(self, out_channels, num_heads=4, numgroups=8):\n",
    "#         super().__init__()\n",
    "#         self.attention_norms = nn.GroupNorm(numgroups, out_channels)\n",
    "#         self.attentions = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "\n",
    "#         # self.head_dim = embed_dim // num_heads from torch source code\n",
    "#         # assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "#         # Get each instance variable\n",
    "#         for key_value in self.attentions.__dict__.items():\n",
    "#             data = key_value[1]\n",
    "#             if isinstance(data, collections.OrderedDict):\n",
    "#                 for k, v in data.items():\n",
    "#                     if torch.is_tensor(v):\n",
    "#                         print('k:', k, ', shape:', v.shape)\n",
    "#             else:\n",
    "#                 print(key_value[0], '=', key_value[1])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = x\n",
    "#         # Attention block of Unet\n",
    "#         batch_size, channels, h, w = out.shape\n",
    "#         in_attn = out.reshape(batch_size, channels, h * w)\n",
    "#         in_attn = self.attention_norms(in_attn)\n",
    "#         in_attn = in_attn.transpose(1, 2)    #So, I guess: [N, (h*w), C] where (h*w) is the target \"sequence length\", and C is the embedding dimension\n",
    "#         out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n",
    "#         print('\\nout_attn shape:', out_attn.shape)\n",
    "#         out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "#         return out_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels = [32, 64, 128, 256, 512]\n",
    "channels = [64, 128, 256, 512, 1024]\n",
    "down_channel_indices = [[0,0], [0,1], [1,2], [2,3]] # in, out indices into channels[] for each down layer\n",
    "heads = 4\n",
    "dim_head = 64\n",
    "b = 1\n",
    "shape = 64\n",
    "for (in_idx, out_idx) in (down_channel_indices):\n",
    "    attn1 = AttentionBlock(channels[in_idx], num_heads=heads, dim_head=dim_head)\n",
    "    c = channels[in_idx]\n",
    "    h = w = shape\n",
    "    x = torch.randn((b, c, h, w))\n",
    "    out = attn1.forward(x)\n",
    "    shape = shape//2\n",
    "    break\n",
    "    print('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "# attn1 = AttentionBlock(emb_dim, heads)\n",
    "# out = attn1.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source code for pytorch's Attention\n",
    "\n",
    "class MultiheadAttention(Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces as described in the paper:\n",
    "    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
    "\n",
    "    Multi-Head Attention is defined as:\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "\n",
    "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
    "\n",
    "    ``nn.MultiHeadAttention`` will use the optimized implementations of\n",
    "    ``scaled_dot_product_attention()`` when possible.\n",
    "\n",
    "    In addition to support for the new ``scaled_dot_product_attention()``\n",
    "    function, for speeding up Inference, MHA will use\n",
    "    fastpath inference with support for Nested Tensors, iff:\n",
    "\n",
    "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
    "    - inputs are batched (3D) with ``batch_first==True``\n",
    "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
    "    - training is disabled (using ``.eval()``)\n",
    "    - ``add_bias_kv`` is ``False``\n",
    "    - ``add_zero_attn`` is ``False``\n",
    "    - ``batch_first`` is ``True`` and the input is batched\n",
    "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
    "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
    "      nor ``attn_mask`` is passed\n",
    "    - autocast is disabled\n",
    "\n",
    "    If the optimized inference fastpath implementation is in use, a\n",
    "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
    "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
    "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
    "    will be returned, and an additional speedup proportional to the fraction of the input\n",
    "    that is padding can be expected.\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Total dimension of the model.\n",
    "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
    "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
    "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
    "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
    "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
    "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
    "            Default: ``False``.\n",
    "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
    "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
    "         https://arxiv.org/abs/2205.14135\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['batch_first']\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
    "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
    "        if embed_dim <= 0 or num_heads <= 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim and num_heads must be greater than 0,\"\n",
    "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
    "            )\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
    "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super().__setstate__(state)\n",
    "\n",
    "\n",
    "#\n",
    "#     The forward method\n",
    "#\n",
    "\n",
    "[docs]    def forward(\n",
    "            self,\n",
    "            query: Tensor,\n",
    "            key: Tensor,\n",
    "            value: Tensor,\n",
    "            key_padding_mask: Optional[Tensor] = None,\n",
    "            need_weights: bool = True,\n",
    "            attn_mask: Optional[Tensor] = None,\n",
    "            average_attn_weights: bool = True,\n",
    "            is_causal : bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n",
    "            or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n",
    "            :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n",
    "            Queries are compared against key-value pairs to produce the output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n",
    "            or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n",
    "            :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n",
    "            ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n",
    "            sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n",
    "            to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n",
    "            Binary and float masks are supported.\n",
    "            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n",
    "            the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\n",
    "        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n",
    "            Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``\n",
    "            and achieve the best performance for MHA.\n",
    "            Default: ``True``.\n",
    "        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n",
    "            :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n",
    "            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n",
    "            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n",
    "            Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\n",
    "            corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n",
    "            the attention weight.\n",
    "            If both attn_mask and key_padding_mask are supplied, their types should match.\n",
    "        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "            heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "            effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n",
    "        is_causal: If specified, applies a causal mask as attention mask.\n",
    "            Default: ``False``.\n",
    "            Warning:\n",
    "            ``is_causal`` provides a hint that ``attn_mask`` is the\n",
    "            causal mask. Providing incorrect hints can result in\n",
    "            incorrect execution, including forward and backward\n",
    "            compatibility.\n",
    "\n",
    "    Outputs:\n",
    "        - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n",
    "          :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n",
    "          where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n",
    "          embedding dimension ``embed_dim``.\n",
    "        - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n",
    "          returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
    "          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
    "          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n",
    "\n",
    "        .. note::\n",
    "            `batch_first` argument is ignored for unbatched inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        why_not_fast_path = ''\n",
    "        if ((attn_mask is not None and torch.is_floating_point(attn_mask))\n",
    "           or (key_padding_mask is not None) and torch.is_floating_point(key_padding_mask)):\n",
    "            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n",
    "\n",
    "        is_batched = query.dim() == 3\n",
    "\n",
    "        key_padding_mask = F._canonical_mask(\n",
    "            mask=key_padding_mask,\n",
    "            mask_name=\"key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(attn_mask),\n",
    "            other_name=\"attn_mask\",\n",
    "            target_type=query.dtype\n",
    "        )\n",
    "\n",
    "        attn_mask = F._canonical_mask(\n",
    "            mask=attn_mask,\n",
    "            mask_name=\"attn_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=query.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "\n",
    "        if not is_batched:\n",
    "            why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
    "        elif query is not key or key is not value:\n",
    "            # When lifting this restriction, don't forget to either\n",
    "            # enforce that the dtypes all match or test cases where\n",
    "            # they don't!\n",
    "            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
    "        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
    "        elif self.in_proj_weight is None:\n",
    "            why_not_fast_path = \"in_proj_weight was None\"\n",
    "        elif query.dtype != self.in_proj_weight.dtype:\n",
    "            # this case will fail anyway, but at least they'll get a useful error message.\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
    "        elif self.training:\n",
    "            why_not_fast_path = \"training is enabled\"\n",
    "        elif (self.num_heads % 2) != 0:\n",
    "            why_not_fast_path = \"self.num_heads is not even\"\n",
    "        elif not self.batch_first:\n",
    "            why_not_fast_path = \"batch_first was not True\"\n",
    "        elif self.bias_k is not None:\n",
    "            why_not_fast_path = \"self.bias_k was not None\"\n",
    "        elif self.bias_v is not None:\n",
    "            why_not_fast_path = \"self.bias_v was not None\"\n",
    "        elif self.add_zero_attn:\n",
    "            why_not_fast_path = \"add_zero_attn was enabled\"\n",
    "        elif not self._qkv_same_embed_dim:\n",
    "            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
    "        elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n",
    "            why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n",
    "                                 is not supported with NestedTensor input\"\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_fast_path:\n",
    "            tensor_args = (\n",
    "                query,\n",
    "                key,\n",
    "                value,\n",
    "                self.in_proj_weight,\n",
    "                self.in_proj_bias,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "            )\n",
    "            # We have to use list comprehensions below because TorchScript does not support\n",
    "            # generator expressions.\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif _is_make_fx_tracing():\n",
    "                why_not_fast_path = \"we are running make_fx tracing\"\n",
    "            elif not all(_check_arg_device(x) for x in tensor_args):\n",
    "                why_not_fast_path = (\"some Tensor argument's device is neither one of \"\n",
    "                                     f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\")\n",
    "            elif torch.is_grad_enabled() and any(_arg_requires_grad(x) for x in tensor_args):\n",
    "                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                     \"input/output projection weights or biases requires_grad\")\n",
    "            if not why_not_fast_path:\n",
    "                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n",
    "\n",
    "                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n",
    "                    return torch._native_multi_head_attention(\n",
    "                        query,\n",
    "                        key,\n",
    "                        value,\n",
    "                        self.embed_dim,\n",
    "                        self.num_heads,\n",
    "                        self.in_proj_weight,\n",
    "                        self.in_proj_bias,\n",
    "                        self.out_proj.weight,\n",
    "                        self.out_proj.bias,\n",
    "                        merged_mask,\n",
    "                        need_weights,\n",
    "                        average_attn_weights,\n",
    "                        mask_type)\n",
    "\n",
    "        any_nested = query.is_nested or key.is_nested or value.is_nested\n",
    "        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
    "                                f\"The fast path was not hit because {why_not_fast_path}\")\n",
    "\n",
    "        if self.batch_first and is_batched:\n",
    "            # make sure that the transpose op does not affect the \"is\" property\n",
    "            if key is value:\n",
    "                if query is key:\n",
    "                    query = key = value = query.transpose(1, 0)\n",
    "                else:\n",
    "                    query, key = (x.transpose(1, 0) for x in (query, key))\n",
    "                    value = key\n",
    "            else:\n",
    "                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask,\n",
    "                use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight,\n",
    "                average_attn_weights=average_attn_weights,\n",
    "                is_causal=is_causal)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights,\n",
    "                attn_mask=attn_mask,\n",
    "                average_attn_weights=average_attn_weights,\n",
    "                is_causal=is_causal)\n",
    "        if self.batch_first and is_batched:\n",
    "            return attn_output.transpose(1, 0), attn_output_weights\n",
    "        else:\n",
    "            return attn_output, attn_output_weights\n",
    "\n",
    "def multi_head_attention_forward(query: Tensor,\n",
    "                                 key: Tensor,\n",
    "                                 value: Tensor,\n",
    "                                 embed_dim_to_check: int,\n",
    "                                 num_heads: int,\n",
    "                                 in_proj_weight: Tensor,\n",
    "                                 in_proj_bias: Tensor,\n",
    "                                 bias_k: Optional[Tensor],\n",
    "                                 bias_v: Optional[Tensor],\n",
    "                                 add_zero_attn: bool,\n",
    "                                 dropout_p: float,\n",
    "                                 out_proj_weight: Tensor,\n",
    "                                 out_proj_bias: Tensor,\n",
    "                                 training: bool = True,\n",
    "                                 key_padding_mask: Optional[Tensor] = None,\n",
    "                                 need_weights: bool = True,\n",
    "                                 attn_mask: Optional[Tensor] = None,\n",
    "                                 use_separate_proj_weight: bool = False,\n",
    "                                 q_proj_weight: Optional[Tensor] = None,\n",
    "                                 k_proj_weight: Optional[Tensor] = None,\n",
    "                                 v_proj_weight: Optional[Tensor] = None,\n",
    "                                 static_k: Optional[Tensor] = None,\n",
    "                                 static_v: Optional[Tensor] = None\n",
    "                                 ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting():\n",
    "        tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v,\n",
    "                    out_proj_weight, out_proj_bias)\n",
    "        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n",
    "            return handle_torch_function(\n",
    "                multi_head_attention_forward, tens_ops, query, key, value,\n",
    "                embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias,\n",
    "                bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight,\n",
    "                out_proj_bias, training=training, key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights, attn_mask=attn_mask,\n",
    "                use_separate_proj_weight=use_separate_proj_weight,\n",
    "                q_proj_weight=q_proj_weight, k_proj_weight=k_proj_weight,\n",
    "                v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "# shit\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):\n",
    "            # self-attention\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "\n",
    "        elif (key is value or torch.equal(key, value)):\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
