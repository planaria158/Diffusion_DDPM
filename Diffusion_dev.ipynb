{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some misc. code snippets while learning diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from celeba_dataset import CelebA\n",
    "from unet_diffusion import UNet_Diffusion, get_time_embedding\n",
    "from noise_scheduler import LinearNoiseScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (64,64) \n",
    "batch_size = 8 \n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "lns = LinearNoiseScheduler(num_timesteps, beta_start, beta_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "### Experiments that give good results:\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Experiment 1:  Jan 11, 2024\n",
    "\n",
    "-  No augmentations other than horizontal flips (do NOT use Gaussian blur from pytorch packages since this messes up the scheduled noising of the images)\n",
    "-  The AttentionBlock uses nn.MultiheadAttention with number heads = 4\n",
    "-  Attention=True only for self.down_1, self.down_2, self.up_2, self.up_1 (all others false)\n",
    "-  img_shape = (64,64), batch_size=64, two-gpu strategy='ddp_find_unused_parameters_true'\n",
    "-  time_emb dimension = 256\n",
    "-  Epochs = 154 (~220K batches of 64 images)\n",
    "-  num_timesteps = 1000, beta_start = 0.0001, beta_end = 0.02\n",
    "-  Exponential moving average with warmup of 2000 batches\n",
    "-  Adam optimizer, lr = 0.0002, b1 = 0.5, b2 = 0.999\n",
    "-  scheduler = None\n",
    "\n",
    "| <img src=\"images/x0_0_nn.MHSA_154_epochs.png\" alt=\"\" width=\"300\"/>  | <img src=\"images/training_loss_154_epochs_nn_MHSA.png\" alt=\"\" width=\"300\"/>  |\n",
    "|:--:| \n",
    "\n",
    "\n",
    "\n",
    "-  {'img_size': [64, 64], 'time_emb_dim': 256, 'channels': [64, 128, 256, 512, 1024], 'down_attn': [False, True, True, False], 'down_channel_indices': [[0, 0], [0, 1], [1, 2], [2, 3]], 'mid_attn': [False, False, False], 'mid_channel_indices': [[3, 4], [4, 4], [4, 3]], 'up_attn': [True, True, False], 'up_channel_indices': [[3, 2], [2, 1], [1, 0]], 'num_heads': 4}\n",
    "-  {'log_dir': './lightning_logs/', 'batch_size': 64, 'num_epochs': 200, 'restart': False, 'checkpoint_name': 'none', 'log_every_nsteps': 1000, 'accelerator': 'gpu', 'devices': 2, 'save_top_k': \n",
    "10, 'checkpoint_every_n_epochs': 1, 'monitor': 'loss', 'mode': 'min'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Experiment 2: Jan 12, 2024\n",
    "\n",
    "-  The AttentionBlock uses my multiheaded self attention code instead of pytorch's code.\n",
    "-  Number of heads = 4\n",
    "-  No augmentations other than horizontal flips\n",
    "-  Attention=True only for self.down_1, self.down_2, self.up_2, self.up_1 (all others false)\n",
    "-  img_shape = (64,64), batch_size=84, two-gpu strategy='ddp_find_unused_parameters_true'\n",
    "-  time_emb dimension = 256\n",
    "-  Epochs 105, (~150K batches of 64 images)\n",
    "-  num_timesteps = 1000, beta_start = 0.0001, beta_end = 0.02\n",
    "-  Exponential moving average with warmup of 2000 batches\n",
    "-  Adam optimizer, lr = 0.0001, b1 = 0.5, b2 = 0.999\n",
    "-  scheduler = none\n",
    "\n",
    "| <img src=\"images/x0_0_my_MHSA_105_epochs.png\" alt=\"\" width=\"300\"/> | <img src=\"images/training_loss_105_epochs_my_MHSA.png\" alt=\"\" width=\"300\"/> | \n",
    "|:--:|\n",
    "\n",
    "-  {'img_size': [64, 64], 'time_emb_dim': 256, 'channels': [64, 128, 256, 512, 1024], 'down_attn': [False, True, True, False], 'down_channel_indices': [[0, 0], [0, 1], [1, 2], [2, 3]], 'mid_attn': [False, False, False], 'mid_channel_indices': [[3, 4], [4, 4], [4, 3]], 'up_attn': [True, True, False], 'up_channel_indices': [[3, 2], [2, 1], [1, 0]], 'num_heads': 4}\n",
    "-  {'log_dir': './lightning_logs/', 'batch_size': 64, 'num_epochs': 300, 'restart': False, 'checkpoint_name': 'none', 'log_every_nsteps': 1000, 'accelerator': 'gpu', 'devices': 2, 'save_top_k': 10, 'checkpoint_every_n_epochs': 1, 'monitor': 'loss', 'mode': 'min'}\n",
    "-  {'train_path': '../data/img_align_celeba/img_align_celeba/train/', 'valid_path': '../data/img_align_celeba/img_align_celeba/valid/', 'limit_size': False, 'size_limit': -1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "### Experiment 3: Jan 13, 2024\n",
    "\n",
    "Same as Experiment 2, but with dropout = 0.1  Background color seems \"better\"\n",
    "\n",
    " <img src=\"images/x0_0_my_MHSA_dropout_0p1_147_epochs.png\" alt=\"\" width=\"300\"/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "### Experiment 4: Jan 20, 2024  \n",
    "\n",
    "Same as Experiment 3, but now allow for attention head dimension to be specified for each layer in the network.\n",
    "\n",
    "-  img_size: [64,64]\n",
    "-  num_heads: 12\n",
    "-  {'dataset_params': {'train_path': '../data/img_align_celeba/img_align_celeba/train/', 'valid_path': '../data/img_align_celeba/img_align_celeba/valid/', 'limit_size': False, 'size_limit': -1}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'img_size': [64, 64], 'time_emb_dim': 256, 'channels': [64, 128, 256, 512, 1024], 'down_attn': [False, True, True, False], 'down_channel_indices': [[0, 0], [0, 1], [1, 2], [2, 3]], 'down_head_sizes': [32, 64, 128, 256], 'mid_attn': [False, False, False], 'mid_channel_indices': [[3, 4], [4, 4], [4, 3]], 'mid_head_sizes': [64, 64, 64], 'up_attn': [True, True, False], 'up_channel_indices': [[3, 2], [2, 1], [1, 0]], 'up_head_sizes': [128, 64, 32], 'num_heads': 12, 'dropout': 0.1, 'attn_dropout': 0.1}, 'train_params': {'log_dir': './lightning_logs/', 'batch_size': 24, 'num_epochs': 200, 'restart': False, 'checkpoint_name': 'None', 'log_every_nsteps': 1000, 'accelerator': 'gpu', 'devices': 2, 'save_top_k': 5, 'checkpoint_every_n_epochs': 1, 'monitor': 'loss', 'mode': 'min'}}\n",
    "\n",
    "### Result: meh...  no better than the simpler approach with single attn head size for all blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "### Experiment 5: Jan 20, 2024  \n",
    "\n",
    "-  img_size: [96,96]\n",
    "-  num_heads: 4\n",
    "-  dim_head: 64\n",
    "-  batch_size: 18\n",
    "-  {'dataset_params': {'train_path': '../data/img_align_celeba/img_align_celeba/train/', 'valid_path': '../data/img_align_celeba/img_align_celeba/valid/', 'limit_size': False, 'size_limit': -1}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'img_size': [96, 96], 'time_emb_dim': 256, 'channels': [64, 128, 256, 512, 1024], 'down_attn': [False, True, True, False], 'down_channel_indices': [[0, 0], [0, 1], [1, 2], [2, 3]], 'mid_attn': [False, False, False], 'mid_channel_indices': [[3, 4], [4, 4], [4, 3]], 'up_attn': [True, True, False], 'up_channel_indices': [[3, 2], [2, 1], [1, 0]], 'num_heads': 4, 'dim_head': 64, 'dropout': 0.1, 'attn_dropout': 0.1}, 'train_params': {'log_dir': './lightning_logs/', 'batch_size': 18, 'num_epochs': 200, 'restart': False, 'checkpoint_name': 'None', 'log_every_nsteps': 1000, 'accelerator': 'gpu', 'devices': 2, 'save_top_k': 10, 'checkpoint_every_n_epochs': 1, 'monitor': 'loss', 'mode': 'min'}}\n",
    "\n",
    "### Meh!  no discernable face-like images generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "### try using nn.MultiheadSelfAttention code instead of mine \n",
    "\n",
    "-  image size = [96,96]\n",
    "-  num heads: 4, result: similar to my failed experiments below.  Gen images contain no face-like data.\n",
    "-  num heads: 8, same result as num_heads=4.  No face-like data generated.\n",
    "\n",
    "-  image_size: [32,32], num heads: 8\n",
    "\n",
    "\n",
    "\n",
    "#### Maybe for larger images, attention is needed at more layers in the UNet?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Jan 23, 2024:  Try with varying levels of attention\n",
    "\n",
    "- img_size: [64,64]\n",
    "- num_heads : 4\n",
    "- dim_head : 64\n",
    "\n",
    "#### No attention\n",
    " <img src=\"images/x0_0_no_attention.png\" alt=\"\" title=\"No Attention\" width=\"400\"/>\n",
    " \n",
    " #### Attention in Downblock layer 2 only (32x32 feature map size)\n",
    " -  batch_size: 90\n",
    " \n",
    " <img src=\"images/x0_0_attn_down_layer2.png\" alt=\"\" width=\"400\"/> \n",
    "\n",
    " #### Attention in Downblock layer 3 only (16x16 feature map size)\n",
    " -  batch_size: 256 \n",
    "\n",
    " <img src=\"images/blahblah.png\" alt=\"\" width=\"400\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "### Did not work\n",
    "-  image size = (96,96) and (128,128), 12 heads of size 64.  Training loss converged nicely.  Generated images contained no face-like data\n",
    "\n",
    "| <img src=\"images/x0_0_96by96_12heads_bad_result.png\" alt=\"\" width=\"300\"> |\n",
    "|:--:|\n",
    "\n",
    "<hr>\n",
    "\n",
    "### TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "time_emb_dim = 256 #128\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "from torchvision.utils import make_grid\n",
    "from unet_diffusion import UNet_Diffusion\n",
    "from diffusion_lightning import DDPM\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_samples = 36\n",
    "num_grid_rows = 6\n",
    "im_channels = 3\n",
    "im_size = img_size[0]\n",
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "task_name = 'default'\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def sample(model, scheduler):\n",
    "    \"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    We save the x0 predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # # Create two random vectors and interpolate between them.\n",
    "    # rand_a = torch.randn(im_channels, im_size, im_size)\n",
    "    # rand_b = torch.randn(im_channels, im_size, im_size)\n",
    "    # delta_ab = rand_a - rand_b\n",
    "    # samples = []\n",
    "    # samples.append(rand_a)\n",
    "    # delt = 1.0/num_samples\n",
    "    # for i in range(1, (num_samples-1), 1):\n",
    "    #     s = rand_a + (i * delt) * delta_ab\n",
    "    #     samples.append(s)\n",
    "\n",
    "    # samples.append(rand_b)\n",
    "    # xt = torch.stack(samples).to(device)\n",
    "    # print('xt shape:', xt.shape)\n",
    "\n",
    "    xt = torch.randn((num_samples, im_channels, im_size, im_size)).to(device)\n",
    "\n",
    "    for i in tqdm(reversed(range(num_timesteps))):\n",
    "        # Get prediction of noise\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "        \n",
    "        # Save x0 every 200th time.\n",
    "        if i % 200 == 0 or (i == num_timesteps-1):\n",
    "            ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "            ims = (ims + 1) / 2\n",
    "            grid = make_grid(ims, nrow=num_grid_rows)\n",
    "            img = torchvision.transforms.ToPILImage()(grid)\n",
    "            if not os.path.exists(os.path.join(task_name, 'samples')):\n",
    "                os.mkdir(os.path.join(task_name, 'samples'))\n",
    "            img.save(os.path.join(task_name, 'samples', 'x0_{}.png'.format(i)))\n",
    "            img.close()\n",
    "\n",
    "\n",
    "def infer():\n",
    "    model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_5/checkpoints/epoch=412-step=418369.ckpt')\n",
    "    model.ema_model = None # dump the extra EMA model used at train-time only\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    print('Model has:', int(total_params//1e6), 'M parameters')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=num_timesteps,\n",
    "                                     beta_start=beta_start,\n",
    "                                     beta_end=beta_end)\n",
    "    with torch.no_grad():\n",
    "        sample(model.model, scheduler)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Run the inference\n",
    "#----------------------------------------------------\n",
    "infer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = torchvision.datasets.Flowers102(root='../data/', split='train', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Misc debugging code for the AttentionBlock (pytorch's vs. the hand-crafted version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dim_head=64, numgroups=8, dropout=0.):  \n",
    "        super().__init__()        \n",
    "        inner_dim = dim_head * num_heads\n",
    "        # dim_head = dim // num_heads\n",
    "        # inner_dim = dim \n",
    "        project_out = not (num_heads == 1 and dim_head == dim)\n",
    "        self.heads = num_heads\n",
    "        self.attention_norm = nn.GroupNorm(numgroups, dim)\n",
    "        self.scale = float(dim_head) ** -0.5\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        in_attn = x.reshape(b, c, h * w)\n",
    "        # GroupNorm applies only to the c channels, so the dimensions of the tensor \n",
    "        # after that is probably not important either way\n",
    "        in_attn = self.attention_norm(in_attn) \n",
    "        in_attn = in_attn.transpose(1,2)\n",
    "        qkv = self.to_qkv(in_attn).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "        return out \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import collections\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, out_channels, num_heads=4, numgroups=8):\n",
    "        super().__init__()\n",
    "        self.attention_norms = nn.GroupNorm(numgroups, out_channels)\n",
    "        self.attentions = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "\n",
    "        # self.head_dim = embed_dim // num_heads from torch source code\n",
    "        # assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Get each instance variable\n",
    "        for key_value in self.attentions.__dict__.items():\n",
    "            data = key_value[1]\n",
    "            if isinstance(data, collections.OrderedDict):\n",
    "                for k, v in data.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        print('k:', k, ', shape:', v.shape)\n",
    "            else:\n",
    "                print(key_value[0], '=', key_value[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        # Attention block of Unet\n",
    "        batch_size, channels, h, w = out.shape\n",
    "        in_attn = out.reshape(batch_size, channels, h * w)\n",
    "        in_attn = self.attention_norms(in_attn)\n",
    "        in_attn = in_attn.transpose(1, 2)    #So, I guess: [N, (h*w), C] where (h*w) is the target \"sequence length\", and C is the embedding dimension\n",
    "        out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n",
    "        print('\\nout_attn shape:', out_attn.shape)\n",
    "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "        return out_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "heads = 4\n",
    "dim_head =128\n",
    "inner_dim = dim_head *  heads\n",
    "numgroups = 8\n",
    "\n",
    "x = torch.randn([2, 256, 32, 32])\n",
    "print('in x shape:', x.shape)\n",
    "\n",
    "b, c, h, w = x.shape\n",
    "norm = nn.GroupNorm(numgroups, dim)\n",
    "in_attn = norm(x)\n",
    "in_attn = x.reshape(b, h * w, c)\n",
    "# in_attn = in_attn.transpose(1, 2)  # reshape to [b, (h*w), c] i.e. [b, seq, emb_dim]\n",
    "print('in_attn shape:', in_attn.shape)\n",
    "\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.normal_(to_qkv.weight.data, mean=0., std=np.sqrt(2 / (dim+inner_dim)))\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "nn.init.xavier_normal_(to_qkv.weight.data)\n",
    "print('to_qkv, mean:', torch.mean(to_qkv.weight.data), ', std:', torch.std(to_qkv.weight.data))\n",
    "\n",
    "\n",
    "qkv = to_qkv(in_attn)\n",
    "print('out shape:', qkv.shape)\n",
    "\n",
    "qkv = qkv.chunk(3, dim = -1)\n",
    "print('q shape:', qkv[0].shape)\n",
    "\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print('q shape:', q.shape)\n",
    "\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) \n",
    "print('dots shape:', dots.shape)\n",
    "\n",
    "out = torch.matmul(dots, v)\n",
    "print('1 out shape:', out.shape)\n",
    "\n",
    "out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "print('2 out shape:', out.shape)\n",
    "\n",
    "to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "out = to_out(out)\n",
    "print('3 out shape:', out.shape)\n",
    "\n",
    "out = out.transpose(1, 2).reshape(b, c, h, w)\n",
    "print('4 out shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "emb_dim = c = 64\n",
    "h = 32\n",
    "w = 32\n",
    "groups = 8\n",
    "heads = 6\n",
    "dim_head = 64 #emb_dim//heads\n",
    "dropout = 0\n",
    "x = torch.randn((b, c, h, w))\n",
    "print('input shape:', x.shape)\n",
    "\n",
    "attn1 = AttentionBlock_new(emb_dim, heads)\n",
    "print('\\n\\n\\n')\n",
    "# attn2 = AttentionBlock(c, heads, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces as described in the paper:\n",
    "    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
    "\n",
    "    Multi-Head Attention is defined as:\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "\n",
    "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
    "\n",
    "    ``nn.MultiHeadAttention`` will use the optimized implementations of\n",
    "    ``scaled_dot_product_attention()`` when possible.\n",
    "\n",
    "    In addition to support for the new ``scaled_dot_product_attention()``\n",
    "    function, for speeding up Inference, MHA will use\n",
    "    fastpath inference with support for Nested Tensors, iff:\n",
    "\n",
    "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
    "    - inputs are batched (3D) with ``batch_first==True``\n",
    "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
    "    - training is disabled (using ``.eval()``)\n",
    "    - ``add_bias_kv`` is ``False``\n",
    "    - ``add_zero_attn`` is ``False``\n",
    "    - ``batch_first`` is ``True`` and the input is batched\n",
    "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
    "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
    "      nor ``attn_mask`` is passed\n",
    "    - autocast is disabled\n",
    "\n",
    "    If the optimized inference fastpath implementation is in use, a\n",
    "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
    "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
    "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
    "    will be returned, and an additional speedup proportional to the fraction of the input\n",
    "    that is padding can be expected.\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Total dimension of the model.\n",
    "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
    "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
    "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
    "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
    "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
    "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
    "            Default: ``False``.\n",
    "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
    "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
    "         https://arxiv.org/abs/2205.14135\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['batch_first']\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
    "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
    "        if embed_dim <= 0 or num_heads <= 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim and num_heads must be greater than 0,\"\n",
    "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
    "            )\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
    "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super().__setstate__(state)\n",
    "\n",
    "[docs]    def forward(\n",
    "            self,\n",
    "            query: Tensor,\n",
    "            key: Tensor,\n",
    "            value: Tensor,\n",
    "            key_padding_mask: Optional[Tensor] = None,\n",
    "            need_weights: bool = True,\n",
    "            attn_mask: Optional[Tensor] = None,\n",
    "            average_attn_weights: bool = True,\n",
    "            is_causal : bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n",
    "            or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n",
    "            :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n",
    "            Queries are compared against key-value pairs to produce the output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n",
    "            or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n",
    "            :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n",
    "            ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n",
    "            sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n",
    "            to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n",
    "            Binary and float masks are supported.\n",
    "            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n",
    "            the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\n",
    "        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n",
    "            Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``\n",
    "            and achieve the best performance for MHA.\n",
    "            Default: ``True``.\n",
    "        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n",
    "            :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n",
    "            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n",
    "            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n",
    "            Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\n",
    "            corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n",
    "            the attention weight.\n",
    "            If both attn_mask and key_padding_mask are supplied, their types should match.\n",
    "        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "            heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "            effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n",
    "        is_causal: If specified, applies a causal mask as attention mask.\n",
    "            Default: ``False``.\n",
    "            Warning:\n",
    "            ``is_causal`` provides a hint that ``attn_mask`` is the\n",
    "            causal mask. Providing incorrect hints can result in\n",
    "            incorrect execution, including forward and backward\n",
    "            compatibility.\n",
    "\n",
    "    Outputs:\n",
    "        - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n",
    "          :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n",
    "          where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n",
    "          embedding dimension ``embed_dim``.\n",
    "        - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n",
    "          returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
    "          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
    "          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n",
    "\n",
    "        .. note::\n",
    "            `batch_first` argument is ignored for unbatched inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        why_not_fast_path = ''\n",
    "        if ((attn_mask is not None and torch.is_floating_point(attn_mask))\n",
    "           or (key_padding_mask is not None) and torch.is_floating_point(key_padding_mask)):\n",
    "            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n",
    "\n",
    "        is_batched = query.dim() == 3\n",
    "\n",
    "        key_padding_mask = F._canonical_mask(\n",
    "            mask=key_padding_mask,\n",
    "            mask_name=\"key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(attn_mask),\n",
    "            other_name=\"attn_mask\",\n",
    "            target_type=query.dtype\n",
    "        )\n",
    "\n",
    "        attn_mask = F._canonical_mask(\n",
    "            mask=attn_mask,\n",
    "            mask_name=\"attn_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=query.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "\n",
    "        if not is_batched:\n",
    "            why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
    "        elif query is not key or key is not value:\n",
    "            # When lifting this restriction, don't forget to either\n",
    "            # enforce that the dtypes all match or test cases where\n",
    "            # they don't!\n",
    "            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
    "        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
    "        elif self.in_proj_weight is None:\n",
    "            why_not_fast_path = \"in_proj_weight was None\"\n",
    "        elif query.dtype != self.in_proj_weight.dtype:\n",
    "            # this case will fail anyway, but at least they'll get a useful error message.\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
    "        elif self.training:\n",
    "            why_not_fast_path = \"training is enabled\"\n",
    "        elif (self.num_heads % 2) != 0:\n",
    "            why_not_fast_path = \"self.num_heads is not even\"\n",
    "        elif not self.batch_first:\n",
    "            why_not_fast_path = \"batch_first was not True\"\n",
    "        elif self.bias_k is not None:\n",
    "            why_not_fast_path = \"self.bias_k was not None\"\n",
    "        elif self.bias_v is not None:\n",
    "            why_not_fast_path = \"self.bias_v was not None\"\n",
    "        elif self.add_zero_attn:\n",
    "            why_not_fast_path = \"add_zero_attn was enabled\"\n",
    "        elif not self._qkv_same_embed_dim:\n",
    "            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
    "        elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n",
    "            why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n",
    "                                 is not supported with NestedTensor input\"\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_fast_path:\n",
    "            tensor_args = (\n",
    "                query,\n",
    "                key,\n",
    "                value,\n",
    "                self.in_proj_weight,\n",
    "                self.in_proj_bias,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "            )\n",
    "            # We have to use list comprehensions below because TorchScript does not support\n",
    "            # generator expressions.\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif _is_make_fx_tracing():\n",
    "                why_not_fast_path = \"we are running make_fx tracing\"\n",
    "            elif not all(_check_arg_device(x) for x in tensor_args):\n",
    "                why_not_fast_path = (\"some Tensor argument's device is neither one of \"\n",
    "                                     f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\")\n",
    "            elif torch.is_grad_enabled() and any(_arg_requires_grad(x) for x in tensor_args):\n",
    "                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                     \"input/output projection weights or biases requires_grad\")\n",
    "            if not why_not_fast_path:\n",
    "                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n",
    "\n",
    "                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n",
    "                    return torch._native_multi_head_attention(\n",
    "                        query,\n",
    "                        key,\n",
    "                        value,\n",
    "                        self.embed_dim,\n",
    "                        self.num_heads,\n",
    "                        self.in_proj_weight,\n",
    "                        self.in_proj_bias,\n",
    "                        self.out_proj.weight,\n",
    "                        self.out_proj.bias,\n",
    "                        merged_mask,\n",
    "                        need_weights,\n",
    "                        average_attn_weights,\n",
    "                        mask_type)\n",
    "\n",
    "        any_nested = query.is_nested or key.is_nested or value.is_nested\n",
    "        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
    "                                f\"The fast path was not hit because {why_not_fast_path}\")\n",
    "\n",
    "        if self.batch_first and is_batched:\n",
    "            # make sure that the transpose op does not affect the \"is\" property\n",
    "            if key is value:\n",
    "                if query is key:\n",
    "                    query = key = value = query.transpose(1, 0)\n",
    "                else:\n",
    "                    query, key = (x.transpose(1, 0) for x in (query, key))\n",
    "                    value = key\n",
    "            else:\n",
    "                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask,\n",
    "                use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight,\n",
    "                average_attn_weights=average_attn_weights,\n",
    "                is_causal=is_causal)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights,\n",
    "                attn_mask=attn_mask,\n",
    "                average_attn_weights=average_attn_weights,\n",
    "                is_causal=is_causal)\n",
    "        if self.batch_first and is_batched:\n",
    "            return attn_output.transpose(1, 0), attn_output_weights\n",
    "        else:\n",
    "            return attn_output, attn_output_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_forward(query: Tensor,\n",
    "                                 key: Tensor,\n",
    "                                 value: Tensor,\n",
    "                                 embed_dim_to_check: int,\n",
    "                                 num_heads: int,\n",
    "                                 in_proj_weight: Tensor,\n",
    "                                 in_proj_bias: Tensor,\n",
    "                                 bias_k: Optional[Tensor],\n",
    "                                 bias_v: Optional[Tensor],\n",
    "                                 add_zero_attn: bool,\n",
    "                                 dropout_p: float,\n",
    "                                 out_proj_weight: Tensor,\n",
    "                                 out_proj_bias: Tensor,\n",
    "                                 training: bool = True,\n",
    "                                 key_padding_mask: Optional[Tensor] = None,\n",
    "                                 need_weights: bool = True,\n",
    "                                 attn_mask: Optional[Tensor] = None,\n",
    "                                 use_separate_proj_weight: bool = False,\n",
    "                                 q_proj_weight: Optional[Tensor] = None,\n",
    "                                 k_proj_weight: Optional[Tensor] = None,\n",
    "                                 v_proj_weight: Optional[Tensor] = None,\n",
    "                                 static_k: Optional[Tensor] = None,\n",
    "                                 static_v: Optional[Tensor] = None\n",
    "                                 ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting():\n",
    "        tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v,\n",
    "                    out_proj_weight, out_proj_bias)\n",
    "        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n",
    "            return handle_torch_function(\n",
    "                multi_head_attention_forward, tens_ops, query, key, value,\n",
    "                embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias,\n",
    "                bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight,\n",
    "                out_proj_bias, training=training, key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights, attn_mask=attn_mask,\n",
    "                use_separate_proj_weight=use_separate_proj_weight,\n",
    "                q_proj_weight=q_proj_weight, k_proj_weight=k_proj_weight,\n",
    "                v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "# shit\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):\n",
    "            # self-attention\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "\n",
    "        elif (key is value or torch.equal(key, value)):\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1.apply(lambda m: print(type(m).__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = attn1.forward(x)\n",
    "print(out1.shape, ', mean:', torch.mean(out1), ', std:', torch.std(out1))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = attn2.forward(x)\n",
    "print(out2.shape, ', mean:', torch.mean(out2), ', std:', torch.std(out2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "batch = torch.randn([32, 3, 128, 128])\n",
    "make_patches = nn.Conv2d(3, 3, 16, 16, padding=1)\n",
    "\n",
    "patches = make_patches(batch)\n",
    "print('patches shape:', patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "\n",
    "# Verify the mean and the variance: \n",
    "abs(mu - np.mean(s))\n",
    "0.0  # may vary\n",
    "\n",
    "abs(sigma - np.std(s, ddof=1))\n",
    "0.1  # may vary\n",
    "\n",
    "\n",
    "# Display the histogram of the samples, along with the probability density function:\n",
    "count, bins, ignored = plt.hist(s, 30, density=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "from torchvision.transforms.v2 import RandomResizedCrop, RandomRotation, GaussianBlur, RandomErasing\n",
    "\n",
    "from celeba_dataset import CelebA\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Dataset, Dataloader\n",
    "#--------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "image_dir_train = Path('../data/img_align_celeba/img_align_celeba/')\n",
    "\n",
    "img_size = (64,64) \n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "                            RandomHorizontalFlip(p=0.50),\n",
    "                            Resize(img_size, antialias=True)\n",
    "                            ])\n",
    "\n",
    "train_dataset = CelebA(image_dir_train, transform=train_transforms, limit_size=True, size_limit=20)\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=5, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self) : #, mean, std):\n",
    "        pass\n",
    "    def __call__(self, img):\n",
    "        img = (img*127.5) + 127.5\n",
    "        return img\n",
    "    \n",
    "unorm  = UnNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, _  = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(torch.min(images[0]), ', ', torch.max(images[0]))\n",
    "\n",
    "\n",
    "cols = 5\n",
    "rows = 4\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(10, 10))\n",
    "idx = 0\n",
    "for img in (images):  \n",
    "    img = unorm(img).to(torch.uint8).permute(1, 2, 0)\n",
    "    # target = unorm(target).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "    idx += 1\n",
    "    ax = plt.subplot(rows, cols, idx)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    if idx == (cols*rows):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_0, _  = next(iter(train_loader))\n",
    "shape = images_0.shape\n",
    "print(shape)\n",
    "noise = torch.randn(shape[2], shape[3])\n",
    "print(noise.shape)\n",
    "print(images[0:5].shape)\n",
    "\n",
    "imgs_n = lns.add_noise(images[0:1], noise, 50)\n",
    "print(imgs_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 2\n",
    "rows = 1\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(5, 5))\n",
    "idx = 0\n",
    "\n",
    "img   = unorm(images[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "img_n = unorm(imgs_n[0]).to(torch.uint8).permute(1, 2, 0)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img)\n",
    "\n",
    "idx += 1\n",
    "ax = plt.subplot(rows, cols, idx)\n",
    "ax.axis('off')\n",
    "plt.imshow(img_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb_dim = 128\n",
    "time_steps = torch.ones((512)) * 999\n",
    "print(time_steps.shape)\n",
    "\n",
    "blah = time_steps[:, None]\n",
    "print(blah.shape)\n",
    "\n",
    "poo = blah.repeat(1, 128//2)\n",
    "print(poo.shape)\n",
    "\n",
    "\n",
    "t_emb = get_time_embedding(time_steps, time_emb_dim)\n",
    "print(t_emb.shape)\n",
    "print(t_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_diffusion import UNet_Diffusion, get_time_embedding\n",
    "from diffusion_lightning import DDPM, EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_location = {'cuda:0':'cuda:1'}\n",
    "# model = DDPM.load_from_checkpoint(checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_10/checkpoints/epoch=3-step=72936.ckpt',\n",
    "#                                   map_location=map_location) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=  DDPM()\n",
    "trainer = pl.Trainer(accelerator='cpu', devices=1, max_epochs=100) \n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=10,\n",
    "    every_n_epochs=1,\n",
    "    monitor = 'loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "map_location = {'cuda:0':'cuda:1'}\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=\"lightning_logs\", default_hp_metric=False)\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=500,\n",
    "                     logger=logger, log_every_n_steps=1000, callbacks=[checkpoint_callback],\n",
    "                     checkpoint_path='/home/mark/dev/diffusion/lightning_logs/version_10/checkpoints/epoch=3-step=72936.ckpt') \n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
