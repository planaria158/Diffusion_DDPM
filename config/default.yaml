dataset_params:
  train_path: '../data/img_align_celeba/img_align_celeba/train/'
  valid_path: '../data/img_align_celeba/img_align_celeba/valid/'
  limit_size: False
  size_limit: -1

diffusion_params:
  num_timesteps : 1000
  beta_start : 0.0001
  beta_end : 0.02
  num_samples : 36
  num_grid_rows : 6  # i.e. 6x6 = 36 above
  sample_epochs : 5
  task_name : 'default'

model_params:
  img_size : [96,96]
  time_emb_dim : 256
  channels : [64, 128, 256, 512, 1024]  
  down_attn: [False, False, True, True]  # attn at 24x24, 12x12
  down_channel_indices: [[0,0], [0,1], [1,2], [2,3]] # in, out indices into channels[] for each down layer

  mid_attn:  [False, False, False]
  mid_channel_indices: [[3,4], [4,4], [4,3]] # in, out indices into channels[] for each mid layer

  up_attn:   [True, True, False] # attn at 24x24, 12x12
  up_channel_indices: [[3,2], [2,1], [1,0]] # in, out indices into channels[] for each up layer
  num_heads : 4
  dim_head : 32
  dropout: 0.
  attn_dropout: 0.1
  loss_weighting: None #'PP'  # PP == perception-prioritized weighting.  None == no weighting.  


train_params:
  log_dir: './lightning_logs/'
  batch_size: 32
  num_epochs: 500
  checkpoint_name: None #'/home/mark/dev/diffusion/lightning_logs/version_22/checkpoints/epoch=57-step=165300.ckpt'
  log_every_nsteps: 1000
  accelerator: 'gpu'
  devices: 2
  save_top_k: 10
  checkpoint_every_n_epochs: 1
  monitor: 'loss'
  mode: 'min'
  