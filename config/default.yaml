dataset_params:
  train_path: '../data/img_align_celeba/img_align_celeba/train/'
  valid_path: '../data/img_align_celeba/img_align_celeba/valid/'
  limit_size: False
  size_limit: -1

diffusion_params:
  num_timesteps : 1000
  beta_start : 0.0001
  beta_end : 0.02
  num_samples : 49
  num_grid_rows : 7  
  sample_epochs : 5
  task_name : 'default'

model_params:
  img_size : [128,128]
  time_emb_dim : 256
  channels : [64, 128, 256, 512, 1024]  
  down_attn: [True, True, True, True]  # for img_size : [128,128], attn at 32x32 and 16x16
  down_channel_indices: [[0,0], [0,1], [1,2], [2,3]] # in, out indices into channels[] for each down layer

  mid_attn:  [True, True, True]
  mid_channel_indices: [[3,4], [4,4], [4,3]] # in, out indices into channels[] for each mid layer

  up_attn:   [True, True, True] # img_size : [128,128], attn at 32x32 and 16x16
  up_channel_indices: [[3,2], [2,1], [1,0]] # in, out indices into channels[] for each up layer
  num_heads : 4
  dim_head : 32
  dropout: 0.0
  attn_dropout: 0.1
  loss_weighting: None #'PP'  # PP == perception-prioritized weighting.  None == no weighting.  


train_params:
  log_dir: './lightning_logs/'
  batch_size: 16
  accumulate_grad_batches: 1
  num_epochs: 300
  checkpoint_name: None #'/home/mark/dev/diffusion/lightning_logs/version_36/checkpoints/epoch=34-step=12495.ckpt'
  log_every_nsteps: 1000
  accelerator: 'gpu'
  devices: 2
  save_top_k: 10
  checkpoint_every_n_epochs: 1
  monitor: 'loss'
  mode: 'min'
  